"""
Privacy Guard LLM - Prompt Classifier ë©”ì¸ ì‹¤í–‰ íŒŒì¼
2025ë…„ 7ì›” ìµœì‹  ëª¨ë¸ ê¸°ë°˜ ê°œì¸ì •ë³´ ìœ„í—˜ë„ ë¶„ì„

ì‚¬ìš©ë²•:
    python run_prompt_classifier.py --mode all      # ëª¨ë“  ëª¨ë¸ í…ŒìŠ¤íŠ¸
    python run_prompt_classifier.py --mode precision # ì •ë°€ ë¹„êµ ê·¸ë£¹ë§Œ
    python run_prompt_classifier.py --mode speed    # ì†ë„ í…ŒìŠ¤íŠ¸ ê·¸ë£¹ë§Œ
    python run_prompt_classifier.py --mode sample   # ìƒ˜í”Œ ì¼€ì´ìŠ¤ë§Œ
"""

import os
import json
import time
import argparse
from datetime import datetime
from typing import Dict, List

# í”„ë¡œì íŠ¸ ëª¨ë“ˆ import
from llm_configs import LLMConfigManager
from prompt_templates import PromptTemplates
from test_cases import TestCases
from llm_api_client import LLMAPIClient
from result_analyzer import ResultAnalyzer

def setup_logging():
    """ë¡œê¹… ì„¤ì •"""
    import logging

    log_dir = "logs"
    os.makedirs(log_dir, exist_ok=True)

    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(f"{log_dir}/prompt_classifier_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"),
            logging.StreamHandler()
        ]
    )

    return logging.getLogger(__name__)

def run_experiment(mode: str = "all", max_cost: float = 5.0, save_results: bool = True):
    """ì‹¤í—˜ ì‹¤í–‰"""
    logger = setup_logging()

    print("ğŸš€ Privacy Guard LLM - Prompt Classifier ì‹¤í—˜ ì‹œì‘")
    print("=" * 80)
    print(f"ğŸ”§ ì‹¤í–‰ ëª¨ë“œ: {mode}")
    print(f"ğŸ’° ìµœëŒ€ ë¹„ìš©: ${max_cost:.2f}")
    print(f"ğŸ“… ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    # ì„¤ì • ì´ˆê¸°í™”
    config_manager = LLMConfigManager()
    templates = PromptTemplates()
    api_client = LLMAPIClient(max_cost=max_cost)

    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸
    available_models = config_manager.get_available_models()
    if not available_models:
        print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        print("API í‚¤ë¥¼ .env íŒŒì¼ì— ì„¤ì •í•˜ê³  í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.")
        return

    # ì‹¤í—˜ ëª¨ë“œì— ë”°ë¥¸ ëª¨ë¸ ì„ íƒ
    if mode == "precision":
        test_models = config_manager.get_precision_group()
        print(f"ğŸ¯ ì •ë°€ ë¹„êµ ê·¸ë£¹: {test_models}")
    elif mode == "speed":
        test_models = config_manager.get_speed_group()
        print(f"âš¡ ì†ë„ í…ŒìŠ¤íŠ¸ ê·¸ë£¹: {test_models}")
    elif mode == "cost":
        test_models = config_manager.get_cost_efficient_group()
        print(f"ğŸ’° ë¹„ìš© íš¨ìœ¨ ê·¸ë£¹: {test_models}")
    elif mode == "sample":
        test_models = available_models[:3]  # ì²˜ìŒ 3ê°œ ëª¨ë¸ë§Œ
        print(f"ğŸ“‹ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸: {test_models}")
    else:  # mode == "all"
        test_models = available_models
        print(f"ğŸ¤– ì „ì²´ ëª¨ë¸: {test_models}")

    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì„ íƒ
    if mode == "sample":
        test_cases = TestCases.get_sample_cases(5)
    else:
        test_cases = TestCases.get_all_cases()

    print(f"\nğŸ“Š ì‹¤í—˜ ê·œëª¨:")
    print(f"   ğŸ¤– í…ŒìŠ¤íŠ¸ ëª¨ë¸: {len(test_models)}ê°œ")
    print(f"   ğŸ“‹ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {len(test_cases)}ê°œ")
    print(f"   ğŸ§ª ì´ ë¶„ì„ ìˆ˜: {len(test_models) * len(test_cases)}ê°œ")

    # ë¹„ìš© ì¶”ì •
    estimated_cost = len(test_models) * len(test_cases) * 0.01  # ëŒ€ëµì ì¸ ì¶”ì •
    print(f"   ğŸ’° ì˜ˆìƒ ë¹„ìš©: ${estimated_cost:.2f}")

    if estimated_cost > max_cost:
        print(f"âš ï¸  ì˜ˆìƒ ë¹„ìš©ì´ í•œë„ë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤. ê³„ì†í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n)")
        if input().lower() != 'y':
            print("ì‹¤í—˜ì„ ì·¨ì†Œí•©ë‹ˆë‹¤.")
            return

    print(f"\nğŸš€ ì‹¤í—˜ ì‹œì‘...")
    print("-" * 80)

    # ì‹¤í—˜ ì‹¤í–‰
    all_results = []
    start_time = time.time()

    for i, case in enumerate(test_cases, 1):
        print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {i}/{len(test_cases)}: {case['description']}")
        print(f"   ğŸ“„ í…ìŠ¤íŠ¸: {case['text']}")
        print(f"   ğŸ·ï¸  ë„ë©”ì¸: {case['domain']}")
        print(f"   ğŸ¯ ì˜ˆìƒ ìœ„í—˜ë„: {case['expected_risk']}")

        case_results = []

        for model_name in test_models:
            try:
                print(f"   ğŸ¤– {model_name} ë¶„ì„ ì¤‘...")

                # ëª¨ë¸ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
                config = config_manager.get_config(model_name)

                # í”„ë¡¬í”„íŠ¸ ìƒì„±
                prompt = templates.get_all_prompts()[case['domain']].format(text=case['text'])

                # API í˜¸ì¶œ
                result = api_client.analyze_text(
                    text=case['text'],
                    config=config,
                    prompt=prompt,
                    expected_risk=case['expected_risk']
                )

                case_results.append(result)

                # ê²°ê³¼ ì¶œë ¥
                print(f"      ìœ„í—˜ë„: {result['predicted_risk']} (ì ìˆ˜: {result['risk_score']:.3f})")
                print(f"      ì²˜ë¦¬ì‹œê°„: {result['processing_time']:.2f}ì´ˆ")
                print(f"      ë¹„ìš©: ${result['cost']:.4f}")
                print(f"      ì •í™•ë„: {'âœ…' if result['correct'] else 'âŒ'}")

                # API í˜¸ì¶œ ì œí•œ
                time.sleep(0.5)

            except Exception as e:
                logger.error(f"ëª¨ë¸ {model_name} ë¶„ì„ ì‹¤íŒ¨: {e}")
                print(f"      âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
                continue

        all_results.extend(case_results)

        # ì§„í–‰ë¥  ì¶œë ¥
        progress = (i / len(test_cases)) * 100
        print(f"   ğŸ“Š ì§„í–‰ë¥ : {progress:.1f}%")

        # ì¤‘ê°„ ê²°ê³¼ ì €ì¥ (10ê°œ ì¼€ì´ìŠ¤ë§ˆë‹¤)
        if save_results and i % 10 == 0:
            save_intermediate_results(all_results, mode)

    # ì‹¤í—˜ ì™„ë£Œ
    total_time = time.time() - start_time
    print(f"\nğŸ‰ ì‹¤í—˜ ì™„ë£Œ!")
    print(f"â±ï¸  ì´ ì†Œìš” ì‹œê°„: {total_time:.2f}ì´ˆ")
    print(f"ğŸ’° ì´ ë¹„ìš©: ${api_client.total_cost:.4f}")

    # ê²°ê³¼ ë¶„ì„
    analyzer = ResultAnalyzer(all_results)
    analysis_report = analyzer.generate_report()

    # ê²°ê³¼ ì¶œë ¥
    print("\nğŸ“Š ì‹¤í—˜ ê²°ê³¼ ë¶„ì„")
    print("=" * 80)
    print(analysis_report)

    # ê²°ê³¼ ì €ì¥
    if save_results:
        save_final_results(all_results, analysis_report, mode)

    return all_results

def save_intermediate_results(results: List[Dict], mode: str):
    """ì¤‘ê°„ ê²°ê³¼ ì €ì¥"""
    os.makedirs("../results", exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"results/intermediate_{mode}_{timestamp}.json"

    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    print(f"   ğŸ’¾ ì¤‘ê°„ ê²°ê³¼ ì €ì¥: {filename}")

def save_final_results(results: List[Dict], analysis_report: str, mode: str):
    """ìµœì¢… ê²°ê³¼ ì €ì¥"""
    os.makedirs("../results", exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    # JSON ê²°ê³¼ ì €ì¥
    results_filename = f"results/final_{mode}_{timestamp}.json"
    with open(results_filename, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    # ë¶„ì„ ë³´ê³ ì„œ ì €ì¥
    report_filename = f"results/report_{mode}_{timestamp}.md"
    with open(report_filename, 'w', encoding='utf-8') as f:
        f.write(f"# Privacy Guard LLM - ì‹¤í—˜ ê²°ê³¼ ë³´ê³ ì„œ\n\n")
        f.write(f"- ì‹¤í–‰ ëª¨ë“œ: {mode}\n")
        f.write(f"- ì‹¤í–‰ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"- ì´ ë¶„ì„ ìˆ˜: {len(results)}ê°œ\n\n")
        f.write(analysis_report)

    print(f"ğŸ’¾ ìµœì¢… ê²°ê³¼ ì €ì¥:")
    print(f"   ğŸ“„ ê²°ê³¼ ë°ì´í„°: {results_filename}")
    print(f"   ğŸ“‹ ë¶„ì„ ë³´ê³ ì„œ: {report_filename}")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='Privacy Guard LLM - Prompt Classifier')
    parser.add_argument('--mode', choices=['all', 'precision', 'speed', 'cost', 'sample'],
                        default='sample', help='ì‹¤í—˜ ëª¨ë“œ')
    parser.add_argument('--max-cost', type=float, default=5.0, help='ìµœëŒ€ ë¹„ìš© í•œë„')
    parser.add_argument('--no-save', action='store_true', help='ê²°ê³¼ ì €ì¥ ì•ˆí•¨')

    args = parser.parse_args()

    # ì‹¤í—˜ ì‹¤í–‰
    try:
        results = run_experiment(
            mode=args.mode,
            max_cost=args.max_cost,
            save_results=not args.no_save
        )

        print(f"\nâœ… ì‹¤í—˜ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œ!")
        print(f"ğŸ“Š ì´ {len(results)}ê°œ ê²°ê³¼ ìƒì„±")

    except KeyboardInterrupt:
        print(f"\nâ¹ï¸  ì‚¬ìš©ìì— ì˜í•´ ì‹¤í—˜ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"\nâŒ ì‹¤í—˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()