"""
ê²°ê³¼ ë¶„ì„ê¸° - ì‹¤í—˜ ê²°ê³¼ ë¶„ì„ ë° ë³´ê³ ì„œ ìƒì„±
í†µê³„ ë¶„ì„, ì„±ëŠ¥ ë¹„êµ, ì‹œê°í™” ë°ì´í„° ìƒì„±
"""

import json
import numpy as np
import pandas as pd
from typing import Dict, List, Any, Tuple
from datetime import datetime
from collections import defaultdict, Counter
import logging

class ResultAnalyzer:
    """ì‹¤í—˜ ê²°ê³¼ ë¶„ì„ê¸°"""

    def __init__(self, results: List[Dict[str, Any]]):
        self.results = results
        self.logger = logging.getLogger(__name__)
        self.df = self._create_dataframe()

    def _create_dataframe(self) -> pd.DataFrame:
        """ê²°ê³¼ë¥¼ pandas DataFrameìœ¼ë¡œ ë³€í™˜"""
        try:
            df = pd.DataFrame(self.results)
            return df
        except Exception as e:
            self.logger.error(f"DataFrame ìƒì„± ì‹¤íŒ¨: {e}")
            return pd.DataFrame()

    def calculate_overall_metrics(self) -> Dict[str, Any]:
        """ì „ì²´ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°"""
        if self.df.empty:
            return {}

        total_cases = len(self.df)
        correct_cases = len(self.df[self.df['correct'] == True])
        error_cases = len(self.df[self.df['error'] == True])

        metrics = {
            'total_cases': total_cases,
            'correct_cases': correct_cases,
            'error_cases': error_cases,
            'accuracy': correct_cases / total_cases * 100 if total_cases > 0 else 0,
            'error_rate': error_cases / total_cases * 100 if total_cases > 0 else 0,
            'avg_processing_time': self.df['processing_time'].mean(),
            'total_cost': self.df['cost'].sum(),
            'avg_cost_per_case': self.df['cost'].mean(),
            'avg_risk_score': self.df['risk_score'].mean(),
            'total_tokens': self.df['token_count'].sum(),
            'avg_tokens_per_case': self.df['token_count'].mean()
        }

        return metrics

    def analyze_by_model(self) -> Dict[str, Dict[str, Any]]:
        """ëª¨ë¸ë³„ ì„±ëŠ¥ ë¶„ì„"""
        model_stats = {}

        for model_name in self.df['model_name'].unique():
            model_df = self.df[self.df['model_name'] == model_name]

            total_cases = len(model_df)
            correct_cases = len(model_df[model_df['correct'] == True])
            error_cases = len(model_df[model_df['error'] == True])

            model_stats[model_name] = {
                'total_cases': total_cases,
                'correct_cases': correct_cases,
                'error_cases': error_cases,
                'accuracy': correct_cases / total_cases * 100 if total_cases > 0 else 0,
                'error_rate': error_cases / total_cases * 100 if total_cases > 0 else 0,
                'avg_processing_time': model_df['processing_time'].mean(),
                'total_cost': model_df['cost'].sum(),
                'avg_cost_per_case': model_df['cost'].mean(),
                'avg_risk_score': model_df['risk_score'].mean(),
                'total_tokens': model_df['token_count'].sum(),
                'cost_per_accuracy': model_df['cost'].sum() / (correct_cases / total_cases) if correct_cases > 0 else float('inf')
            }

        return model_stats

    def analyze_by_domain(self) -> Dict[str, Dict[str, Any]]:
        """ë„ë©”ì¸ë³„ ì„±ëŠ¥ ë¶„ì„"""
        domain_stats = {}

        for domain in self.df['domain'].unique():
            domain_df = self.df[self.df['domain'] == domain]

            total_cases = len(domain_df)
            correct_cases = len(domain_df[domain_df['correct'] == True])

            domain_stats[domain] = {
                'total_cases': total_cases,
                'correct_cases': correct_cases,
                'accuracy': correct_cases / total_cases * 100 if total_cases > 0 else 0,
                'avg_processing_time': domain_df['processing_time'].mean(),
                'avg_cost': domain_df['cost'].mean(),
                'avg_risk_score': domain_df['risk_score'].mean()
            }

        return domain_stats

    def analyze_by_risk_level(self) -> Dict[str, Dict[str, Any]]:
        """ìœ„í—˜ë„ë³„ ì„±ëŠ¥ ë¶„ì„"""
        risk_stats = {}

        for risk_level in self.df['expected_risk'].unique():
            risk_df = self.df[self.df['expected_risk'] == risk_level]

            total_cases = len(risk_df)
            correct_cases = len(risk_df[risk_df['correct'] == True])

            # ì˜ˆì¸¡ ë¶„í¬
            predicted_distribution = risk_df['predicted_risk'].value_counts().to_dict()

            risk_stats[risk_level] = {
                'total_cases': total_cases,
                'correct_cases': correct_cases,
                'accuracy': correct_cases / total_cases * 100 if total_cases > 0 else 0,
                'predicted_distribution': predicted_distribution,
                'avg_risk_score': risk_df['risk_score'].mean()
            }

        return risk_stats

    def generate_confusion_matrix(self) -> Dict[str, Dict[str, int]]:
        """í˜¼ë™ í–‰ë ¬ ìƒì„±"""
        confusion_matrix = defaultdict(lambda: defaultdict(int))

        for _, row in self.df.iterrows():
            expected = row['expected_risk']
            predicted = row['predicted_risk']
            confusion_matrix[expected][predicted] += 1

        return dict(confusion_matrix)

    def find_misclassified_cases(self) -> List[Dict[str, Any]]:
        """ì˜¤ë¶„ë¥˜ ì‚¬ë¡€ ë¶„ì„"""
        misclassified = self.df[self.df['correct'] == False]

        cases = []
        for _, row in misclassified.iterrows():
            cases.append({
                'text': row['text'],
                'model_name': row['model_name'],
                'expected_risk': row['expected_risk'],
                'predicted_risk': row['predicted_risk'],
                'explanation': row['explanation'],
                'domain': row['domain']
            })

        return cases

    def get_best_performing_models(self) -> Dict[str, str]:
        """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°"""
        model_stats = self.analyze_by_model()

        if not model_stats:
            return {}

        best_models = {
            'highest_accuracy': max(model_stats.items(), key=lambda x: x[1]['accuracy'])[0],
            'fastest_processing': min(model_stats.items(), key=lambda x: x[1]['avg_processing_time'])[0],
            'most_cost_effective': min(model_stats.items(), key=lambda x: x[1]['cost_per_accuracy'])[0],
            'lowest_error_rate': min(model_stats.items(), key=lambda x: x[1]['error_rate'])[0]
        }

        return best_models

    def generate_insights(self) -> List[str]:
        """ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        insights = []

        overall_metrics = self.calculate_overall_metrics()
        model_stats = self.analyze_by_model()
        domain_stats = self.analyze_by_domain()
        risk_stats = self.analyze_by_risk_level()

        # ì „ì²´ ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸
        accuracy = overall_metrics.get('accuracy', 0)
        if accuracy > 80:
            insights.append("âœ… ì „ì²´ì ìœ¼ë¡œ ë†’ì€ ì •í™•ë„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.")
        elif accuracy > 60:
            insights.append("âš ï¸ ì „ì²´ì ìœ¼ë¡œ ë³´í†µ ìˆ˜ì¤€ì˜ ì •í™•ë„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.")
        else:
            insights.append("âŒ ì „ì²´ì ìœ¼ë¡œ ë‚®ì€ ì •í™•ë„ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤. ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")

        # ëª¨ë¸ ì„±ëŠ¥ ì¸ì‚¬ì´íŠ¸
        if model_stats:
            best_model = max(model_stats.items(), key=lambda x: x[1]['accuracy'])
            worst_model = min(model_stats.items(), key=lambda x: x[1]['accuracy'])

            insights.append(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model[0]} (ì •í™•ë„: {best_model[1]['accuracy']:.1f}%)")
            insights.append(f"ğŸ“‰ ê°œì„  í•„ìš” ëª¨ë¸: {worst_model[0]} (ì •í™•ë„: {worst_model[1]['accuracy']:.1f}%)")

        # ë„ë©”ì¸ë³„ ì¸ì‚¬ì´íŠ¸
        if domain_stats:
            domain_accuracies = [(domain, stats['accuracy']) for domain, stats in domain_stats.items()]
            domain_accuracies.sort(key=lambda x: x[1], reverse=True)

            insights.append(f"ğŸ“‹ ë„ë©”ì¸ë³„ ì„±ëŠ¥: {domain_accuracies[0][0]} ë„ë©”ì¸ì´ ê°€ì¥ ë†’ìŒ ({domain_accuracies[0][1]:.1f}%)")
            if len(domain_accuracies) > 1:
                insights.append(f"ğŸ” ê°œì„  í•„ìš” ë„ë©”ì¸: {domain_accuracies[-1][0]} ({domain_accuracies[-1][1]:.1f}%)")

        # ìœ„í—˜ë„ë³„ ì¸ì‚¬ì´íŠ¸
        if risk_stats:
            risk_accuracies = [(risk, stats['accuracy']) for risk, stats in risk_stats.items()]
            risk_accuracies.sort(key=lambda x: x[1], reverse=True)

            insights.append(f"âš ï¸ ìœ„í—˜ë„ë³„ ì„±ëŠ¥: {risk_accuracies[0][0]} ìœ„í—˜ë„ê°€ ê°€ì¥ ì •í™•í•¨ ({risk_accuracies[0][1]:.1f}%)")

        # ë¹„ìš© íš¨ìœ¨ì„± ì¸ì‚¬ì´íŠ¸
        avg_cost = overall_metrics.get('avg_cost_per_case', 0)
        if avg_cost < 0.001:
            insights.append("ğŸ’° ë§¤ìš° ì €ë ´í•œ ë¹„ìš©ìœ¼ë¡œ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        elif avg_cost < 0.01:
            insights.append("ğŸ’µ ì ì •í•œ ë¹„ìš©ìœ¼ë¡œ ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        else:
            insights.append("ğŸ’¸ ë¹„ìš©ì´ ë†’ìœ¼ë‹ˆ íš¨ìœ¨ì ì¸ ëª¨ë¸ ì„ íƒì´ í•„ìš”í•©ë‹ˆë‹¤.")

        return insights

    def generate_report(self) -> str:
        """ì „ì²´ ë¶„ì„ ë¦¬í¬íŠ¸ í…ìŠ¤íŠ¸ ìƒì„±"""
        lines = []

        lines.append("ğŸ“Š Privacy Guard LLM ì‹¤í—˜ ë¶„ì„ ë¦¬í¬íŠ¸")
        lines.append("=" * 60)

        # ì „ì²´ ì§€í‘œ
        lines.append("\n[1] ì „ì²´ ì„±ëŠ¥ ìš”ì•½")
        metrics = self.calculate_overall_metrics()
        if not metrics:
            return "âš ï¸ ê²°ê³¼ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

        lines.append(f"- ì „ì²´ ì¼€ì´ìŠ¤ ìˆ˜: {metrics['total_cases']}")
        lines.append(f"- ì •í™•íˆ ë¶„ë¥˜ëœ ì¼€ì´ìŠ¤: {metrics['correct_cases']}ê°œ")
        lines.append(f"- ì •í™•ë„: {metrics['accuracy']:.1f}%")
        lines.append(f"- í‰ê·  ì²˜ë¦¬ ì‹œê°„: {metrics['avg_processing_time']:.2f}ì´ˆ")
        lines.append(f"- ì´ ë¹„ìš©: ${metrics['total_cost']:.4f}")
        lines.append(f"- í‰ê·  ìœ„í—˜ë„ ì ìˆ˜: {metrics['avg_risk_score']:.3f}")
        lines.append(f"- í‰ê·  ë¹„ìš©: ${metrics['avg_cost_per_case']:.4f}")

        # ëª¨ë¸ë³„ ì„±ëŠ¥
        lines.append("\n[2] ëª¨ë¸ë³„ ì„±ëŠ¥ ìš”ì•½")
        model_stats = self.analyze_by_model()
        for model, stat in model_stats.items():
            lines.append(f"- {model}: ì •í™•ë„ {stat['accuracy']:.1f}%, í‰ê·  ì‹œê°„ {stat['avg_processing_time']:.2f}s, ì´ ë¹„ìš© ${stat['total_cost']:.4f}")

        # ë„ë©”ì¸ë³„ ì„±ëŠ¥
        lines.append("\n[3] ë„ë©”ì¸ë³„ ì„±ëŠ¥ ìš”ì•½")
        domain_stats = self.analyze_by_domain()
        for domain, stat in domain_stats.items():
            lines.append(f"- {domain}: ì •í™•ë„ {stat['accuracy']:.1f}%, í‰ê·  ë¹„ìš© ${stat['avg_cost']:.4f}")

        # ìœ„í—˜ë„ë³„ ì„±ëŠ¥
        lines.append("\n[4] ìœ„í—˜ë„ë³„ ì„±ëŠ¥ ìš”ì•½")
        risk_stats = self.analyze_by_risk_level()
        for level, stat in risk_stats.items():
            lines.append(f"- {level}: ì •í™•ë„ {stat['accuracy']:.1f}%, ë¶„í¬ {stat['predicted_distribution']}")

        # ì¸ì‚¬ì´íŠ¸
        lines.append("\n[5] ì£¼ìš” ì¸ì‚¬ì´íŠ¸")
        insights = self.generate_insights()
        for insight in insights:
            lines.append(f"- {insight}")

        return "\n".join(lines)
