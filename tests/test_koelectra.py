"""
KoELECTRA ê°œì¸ì •ë³´ ë¬¸ë§¥ ì´í•´ í…ŒìŠ¤íŠ¸
"""

import torch
import time
import sys
from transformers import ElectraTokenizer, ElectraModel
import numpy as np

class KoELECTRAPrivacyTester:
    def __init__(self):
        self.model = None
        self.tokenizer = None

    def test_installation(self):
        """KoELECTRA ì„¤ì¹˜ í™•ì¸"""
        try:
            from transformers import ElectraTokenizer, ElectraModel
            print("âœ… KoELECTRA (transformers) ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì„±ê³µ!")
            return True
        except ImportError as e:
            print(f"âŒ KoELECTRA import ì‹¤íŒ¨: {e}")
            return False

    def load_model(self):
        """KoELECTRA ëª¨ë¸ ë¡œë”©"""
        try:
            print("ğŸ”„ KoELECTRA ëª¨ë¸ ë¡œë”© ì¤‘...")
            start_time = time.time()

            # KoELECTRA base ëª¨ë¸ ì‚¬ìš©
            model_name = 'monologg/koelectra-base-v3-discriminator'
            self.tokenizer = ElectraTokenizer.from_pretrained(model_name)
            self.model = ElectraModel.from_pretrained(model_name)
            self.model.eval()

            load_time = time.time() - start_time
            print(f"âœ… KoELECTRA ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {load_time:.2f}ì´ˆ)")
            print(f"ğŸ“Š ëª¨ë¸ ì •ë³´: {model_name}")
            print(f"ğŸ“Š ì–´íœ˜ ì‚¬ì „ í¬ê¸°: {len(self.tokenizer)}")

            return True
        except Exception as e:
            print(f"âŒ KoELECTRA ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            print("ğŸ’¡ ì°¸ê³ : ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•˜ê±°ë‚˜ ë‹¤ë¥¸ ëª¨ë¸ì„ ì‹œë„í•´ë³´ì„¸ìš”")
            return False

    def test_tokenization(self):
        """í† í¬ë‚˜ì´ì§• í…ŒìŠ¤íŠ¸"""
        print("\nğŸ”¤ KoELECTRA í† í¬ë‚˜ì´ì§• í…ŒìŠ¤íŠ¸")
        print("-" * 50)

        test_sentences = [
            "ì•ˆë…•í•˜ì„¸ìš” ê¹€ì² ìˆ˜ì…ë‹ˆë‹¤",
            "ì „í™”ë²ˆí˜¸ëŠ” 010-1234-5678ì…ë‹ˆë‹¤",
            "35ì„¸ ë‚¨ì„± ì˜ì‚¬ì´ê³  ê°•ë‚¨êµ¬ì— ê±°ì£¼í•©ë‹ˆë‹¤",
            "í™˜ì ê¹€ì² ìˆ˜ê°€ ì–´ì œ ìˆ˜ìˆ ì„ ë°›ì•˜ìŠµë‹ˆë‹¤",
            "API í‚¤ëŠ” sk-abc123ì…ë‹ˆë‹¤",
            "ìš°ë¦¬ íšŒì‚¬ Phoenix í”„ë¡œì íŠ¸ ì˜ˆì‚°ì€ 50ì–µì…ë‹ˆë‹¤",
            "í˜ˆë‹¹ 350, ì¼€í†¤ì‚°ì¦ìœ¼ë¡œ ì‘ê¸‰ì‹¤ì— ë‚´ì›í•œ 40ëŒ€ ë‚¨ì„±"
        ]

        for sentence in test_sentences:
            tokens = self.tokenizer.tokenize(sentence)
            token_ids = self.tokenizer.encode(sentence)

            print(f"ì›ë¬¸: {sentence}")
            print(f"í† í°: {tokens}")
            print(f"í† í° ìˆ˜: {len(tokens)}, ID ìˆ˜: {len(token_ids)}")
            print()

    def get_sentence_embedding(self, text):
        """ë¬¸ì¥ ì„ë² ë”© ì¶”ì¶œ"""
        try:
            # í† í¬ë‚˜ì´ì§• ë° ì¸ì½”ë”©
            encoding = self.tokenizer(
                text,
                add_special_tokens=True,
                max_length=128,
                padding='max_length',
                truncation=True,
                return_tensors='pt'
            )

            with torch.no_grad():
                outputs = self.model(**encoding)
                # [CLS] í† í°ì˜ ì„ë² ë”© ì‚¬ìš©
                sentence_embedding = outputs.last_hidden_state[:, 0, :].squeeze()

            return sentence_embedding

        except Exception as e:
            print(f"ì„ë² ë”© ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return None

    def test_similarity(self):
        """ë¬¸ë§¥ ìœ ì‚¬ë„ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ” KoELECTRA ë¬¸ë§¥ ìœ ì‚¬ë„ í…ŒìŠ¤íŠ¸")
        print("-" * 50)

        test_pairs = [
            ("í™˜ì ê¹€ì² ìˆ˜ê°€ ìˆ˜ìˆ ë°›ì•˜ìŠµë‹ˆë‹¤", "ê¹€ì² ìˆ˜ êµìˆ˜ë‹˜ì´ ê°•ì˜í•˜ì…¨ìŠµë‹ˆë‹¤", "í™˜ì vs êµìˆ˜"),
            ("35ì„¸ ë‚¨ì„± ì˜ì‚¬", "40ëŒ€ ì—¬ì„± ê°„í˜¸ì‚¬", "ì˜ë£Œì§„ ì •ë³´"),
            ("API í‚¤ëŠ” sk-abc123", "ë¹„ë°€ë²ˆí˜¸ëŠ” password123", "ì¸ì¦ ì •ë³´"),
            ("í˜ˆë‹¹ 350 ì¼€í†¤ì‚°ì¦", "í˜ˆì•• 140/90 ê³ í˜ˆì••", "ì˜ë£Œ ìˆ˜ì¹˜"),
            ("ìš°ë¦¬ íšŒì‚¬ ë§¤ì¶œ 50ì–µ", "ê²½ìŸì‚¬ ìˆ˜ìµ 30ì–µ", "ê¸°ì—… ì •ë³´"),
            ("ê°•ë‚¨êµ¬ì— ê±°ì£¼í•˜ëŠ” ì˜ì‚¬", "ì„œì´ˆêµ¬ì— ì‚¬ëŠ” ë³€í˜¸ì‚¬", "ê±°ì£¼ì§€+ì§ì—…"),
            ("Phoenix í”„ë¡œì íŠ¸ ì˜ˆì‚°", "Alpha í”„ë¡œì íŠ¸ ì¼ì •", "í”„ë¡œì íŠ¸ ì •ë³´")
        ]

        for text1, text2, desc in test_pairs:
            try:
                emb1 = self.get_sentence_embedding(text1)
                emb2 = self.get_sentence_embedding(text2)

                if emb1 is not None and emb2 is not None:
                    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
                    similarity = torch.cosine_similarity(emb1, emb2, dim=0).item()

                    print(f"ğŸ“Š {desc}")
                    print(f"  í…ìŠ¤íŠ¸1: {text1}")
                    print(f"  í…ìŠ¤íŠ¸2: {text2}")
                    print(f"  ìœ ì‚¬ë„: {similarity:.4f}")

                    if similarity > 0.8:
                        print("  ğŸ”´ ë§¤ìš° ìœ ì‚¬")
                    elif similarity > 0.6:
                        print("  ğŸŸ¡ ì–´ëŠì •ë„ ìœ ì‚¬")
                    elif similarity > 0.4:
                        print("  ğŸŸ¢ ì•½ê°„ ìœ ì‚¬")
                    else:
                        print("  ğŸ”µ ë‚®ì€ ìœ ì‚¬ë„")
                    print()

            except Exception as e:
                print(f"  âŒ ì˜¤ë¥˜: {e}")
                print()

    def test_korean_specific_features(self):
        """í•œêµ­ì–´ íŠ¹í™” ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ‡°ğŸ‡· KoELECTRA í•œêµ­ì–´ íŠ¹í™” ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
        print("-" * 50)

        # í•œêµ­ì–´ íŠ¹í™” í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
        korean_specific_tests = [
            ("ê¹€ì² ìˆ˜ì”¨ê°€ ì˜¤ì…¨ìŠµë‹ˆë‹¤", "ê¹€ì² ìˆ˜ë‹˜ì´ ì˜¤ì…¨ìŠµë‹ˆë‹¤", "í•œêµ­ì–´ ì¡´ëŒ“ë§ ì°¨ì´"),
            ("ë³‘ì›ì— ê°”ì–´ìš”", "ë³‘ì›ì— ê°€ì…¨ì–´ìš”", "ë†’ì„ë²• ì°¨ì´"),
            ("ì˜ì‚¬ì„ ìƒë‹˜", "ì˜ì‚¬ ì„ ìƒë‹˜", "ë„ì–´ì“°ê¸° ì°¨ì´"),
            ("010-1234-5678", "ê³µì¼ê³µ-ì¼ì´ì‚¼ì‚¬-ì˜¤ìœ¡ì¹ íŒ”", "ìˆ«ì í‘œí˜„ ì°¨ì´"),
            ("ê°•ë‚¨êµ¬ ì—­ì‚¼ë™", "ê°•ë‚¨êµ¬ì—­ì‚¼ë™", "ì£¼ì†Œ í‘œê¸° ì°¨ì´"),
            ("ì‚¼ì„±ì „ì", "ì‚¼ì„± ì „ì", "ê¸°ì—…ëª… í‘œê¸°"),
            ("ì½”ë¡œë‚˜19", "ì½”ë¡œë‚˜ 19", "ìˆ«ì í¬í•¨ ë‹¨ì–´")
        ]

        print("ğŸ”— í•œêµ­ì–´ íŠ¹í™” ìœ ì‚¬ë„ í…ŒìŠ¤íŠ¸:")
        for text1, text2, desc in korean_specific_tests:
            try:
                emb1 = self.get_sentence_embedding(text1)
                emb2 = self.get_sentence_embedding(text2)

                if emb1 is not None and emb2 is not None:
                    similarity = torch.cosine_similarity(emb1, emb2, dim=0).item()
                    print(f"  {desc}")
                    print(f"    '{text1}' vs '{text2}'")
                    print(f"    ìœ ì‚¬ë„: {similarity:.4f}")

                    if similarity > 0.9:
                        print("    âœ… ë§¤ìš° ìš°ìˆ˜í•œ í•œêµ­ì–´ ì´í•´")
                    elif similarity > 0.7:
                        print("    âœ… ì¢‹ì€ í•œêµ­ì–´ ì´í•´")
                    elif similarity > 0.5:
                        print("    âš ï¸ ë³´í†µ ìˆ˜ì¤€")
                    else:
                        print("    âŒ í•œêµ­ì–´ ì´í•´ ë¶€ì¡±")
                    print()

            except Exception as e:
                print(f"    âŒ ì˜¤ë¥˜: {e}")

    def test_privacy_detection(self):
        """ê°œì¸ì •ë³´ ê°ì§€ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print("\nğŸ›¡ï¸ KoELECTRA ê°œì¸ì •ë³´ ê°ì§€ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
        print("-" * 50)

        test_cases = [
            ("ì•ˆë…•í•˜ì„¸ìš” ì œ ì´ë¦„ì€ ê¹€ì² ìˆ˜ì…ë‹ˆë‹¤", "ì§ì ‘ ê°œì¸ì •ë³´", "HIGH"),
            ("ì „í™”ë²ˆí˜¸ëŠ” 010-1234-5678ì…ë‹ˆë‹¤", "ì „í™”ë²ˆí˜¸", "HIGH"),
            ("35ì„¸ ë‚¨ì„± ì˜ì‚¬ì´ê³  ê°•ë‚¨êµ¬ì— ê±°ì£¼í•©ë‹ˆë‹¤", "ì¡°í•© ì •ë³´", "MEDIUM"),
            ("í™˜ì ê¹€ì² ìˆ˜ê°€ ì–´ì œ ìˆ˜ìˆ ë°›ì•˜ìŠµë‹ˆë‹¤", "ì˜ë£Œ ë§¥ë½", "HIGH"),
            ("ê¹€ì² ìˆ˜ êµìˆ˜ë‹˜ì´ ê°•ì˜í•˜ì…¨ìŠµë‹ˆë‹¤", "êµìœ¡ ë§¥ë½", "LOW"),
            ("API í‚¤ëŠ” sk-abc123ì…ë‹ˆë‹¤", "ê¸°ìˆ  ì •ë³´", "HIGH"),
            ("ìš°ë¦¬ íšŒì‚¬ Phoenix í”„ë¡œì íŠ¸ ì˜ˆì‚°ì€ 50ì–µì…ë‹ˆë‹¤", "ê¸°ì—… ê¸°ë°€", "HIGH"),
            ("í˜ˆë‹¹ 350, ì¼€í†¤ì‚°ì¦ìœ¼ë¡œ ì‘ê¸‰ì‹¤ì— ë‚´ì›í•œ 40ëŒ€ ë‚¨ì„±", "ì˜ë£Œ ì¡°í•© ì •ë³´", "HIGH"),
            ("ê°•ë‚¨êµ¬ì— ì‚¬ëŠ” 35ì„¸ ë³€í˜¸ì‚¬ê°€ BMWë¥¼ ìš´ì „í•©ë‹ˆë‹¤", "ìƒí™œ ì¡°í•© ì •ë³´", "MEDIUM"),
            ("ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”", "ì¼ë°˜ í…ìŠ¤íŠ¸", "NONE"),
            ("ì ì‹¬ ë­ ë¨¹ì„ê¹Œìš”?", "ì¼ìƒ ëŒ€í™”", "NONE")
        ]

        for text, category, expected_risk in test_cases:
            try:
                embedding = self.get_sentence_embedding(text)

                if embedding is not None:
                    # í•œêµ­ì–´ íŠ¹í™” íœ´ë¦¬ìŠ¤í‹± ìœ„í—˜ë„ ê³„ì‚°
                    privacy_score = 0.0

                    # ì§ì ‘ ê°œì¸ì •ë³´
                    if any(keyword in text for keyword in ['ì´ë¦„', 'ê¹€ì² ìˆ˜', 'ë°•ì˜í¬', 'ì´ì˜í¬']):
                        privacy_score += 0.4
                    if any(keyword in text for keyword in ['010-', '011-', 'ì „í™”ë²ˆí˜¸']):
                        privacy_score += 0.4

                    # ì˜ë£Œ ì •ë³´
                    if any(keyword in text for keyword in ['ì˜ì‚¬', 'í™˜ì', 'ìˆ˜ìˆ ', 'ë³‘ì›']):
                        privacy_score += 0.2
                    if any(keyword in text for keyword in ['í˜ˆë‹¹', 'í˜ˆì••', 'ì¼€í†¤ì‚°ì¦', 'ê³ í˜ˆì••']):
                        privacy_score += 0.3

                    # ê¸°ì—…/ê¸°ìˆ  ì •ë³´
                    if any(keyword in text for keyword in ['API', 'sk-', 'í”„ë¡œì íŠ¸', 'ì˜ˆì‚°']):
                        privacy_score += 0.3
                    if any(keyword in text for keyword in ['íšŒì‚¬', 'ë§¤ì¶œ', 'Phoenix', 'Alpha']):
                        privacy_score += 0.2

                    # ì¡°í•© ì •ë³´
                    age_mentioned = any(keyword in text for keyword in ['ì„¸', 'ì‚´', 'ëŒ€'])
                    location_mentioned = any(keyword in text for keyword in ['ê°•ë‚¨', 'ì„œì´ˆ', 'ê±°ì£¼', 'ì‚¬ëŠ”'])
                    job_mentioned = any(keyword in text for keyword in ['ì˜ì‚¬', 'ë³€í˜¸ì‚¬', 'êµìˆ˜', 'ê°„í˜¸ì‚¬'])

                    combination_count = sum([age_mentioned, location_mentioned, job_mentioned])
                    if combination_count >= 2:
                        privacy_score += 0.2 * combination_count

                    # ìƒí™œ ì •ë³´
                    if any(keyword in text for keyword in ['BMW', 'ì•„íŒŒíŠ¸', 'ìš´ì „']):
                        privacy_score += 0.1

                    print(f"ğŸ“ {category}")
                    print(f"  í…ìŠ¤íŠ¸: {text}")
                    print(f"  ì˜ˆìƒ ìœ„í—˜ë„: {expected_risk}")
                    print(f"  ê³„ì‚°ëœ ì ìˆ˜: {privacy_score:.2f}")
                    print(f"  ì„ë² ë”© ì°¨ì›: {embedding.shape}")

                    # ìœ„í—˜ë„ ë ˆë²¨ íŒì •
                    if privacy_score >= 0.7:
                        calculated_risk = "HIGH"
                    elif privacy_score >= 0.4:
                        calculated_risk = "MEDIUM"
                    elif privacy_score >= 0.1:
                        calculated_risk = "LOW"
                    else:
                        calculated_risk = "NONE"

                    print(f"  íŒì • ìœ„í—˜ë„: {calculated_risk}")

                    if calculated_risk == expected_risk:
                        print("  âœ… ì˜ˆìƒê³¼ ì¼ì¹˜")
                    else:
                        print("  âš ï¸ ì˜ˆìƒê³¼ ë‹¤ë¦„")
                    print()

            except Exception as e:
                print(f"  âŒ ì˜¤ë¥˜: {e}")
                print()

    def test_performance_comparison(self):
        """ì„±ëŠ¥ ë¹„êµ í…ŒìŠ¤íŠ¸"""
        print("\nâš¡ KoELECTRA ì„±ëŠ¥ ì¸¡ì •")
        print("-" * 50)

        test_texts = [
            "ì§§ì€ í…ìŠ¤íŠ¸",
            "ì¡°ê¸ˆ ë” ê¸´ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. 35ì„¸ ë‚¨ì„± ì˜ì‚¬ì´ê³  ê°•ë‚¨êµ¬ì— ê±°ì£¼í•©ë‹ˆë‹¤.",
            "ë§¤ìš° ê¸´ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. " * 20  # ê¸´ í…ìŠ¤íŠ¸ í…ŒìŠ¤íŠ¸
        ]

        for i, text in enumerate(test_texts, 1):
            start_time = time.time()
            embedding = self.get_sentence_embedding(text)
            end_time = time.time()

            if embedding is not None:
                processing_time = (end_time - start_time) * 1000  # ms ë‹¨ìœ„
                print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ {i}:")
                print(f"  í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)}ì")
                print(f"  ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ms")
                print(f"  ì„ë² ë”© í¬ê¸°: {embedding.shape}")
                print()

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸ§ª KoELECTRA ê°œì¸ì •ë³´ ê°ì§€ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)

    tester = KoELECTRAPrivacyTester()

    # 1. ì„¤ì¹˜ í™•ì¸
    if not tester.test_installation():
        print("âŒ KoELECTRA ì„¤ì¹˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        sys.exit(1)

    # 2. ëª¨ë¸ ë¡œë”©
    if not tester.load_model():
        print("âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
        sys.exit(1)

    # 3. í† í¬ë‚˜ì´ì§• í…ŒìŠ¤íŠ¸
    tester.test_tokenization()

    # 4. í•œêµ­ì–´ íŠ¹í™” ê¸°ëŠ¥ í…ŒìŠ¤íŠ¸
    tester.test_korean_specific_features()

    # 5. ìœ ì‚¬ë„ í…ŒìŠ¤íŠ¸
    tester.test_similarity()

    # 6. ê°œì¸ì •ë³´ ê°ì§€ í…ŒìŠ¤íŠ¸
    tester.test_privacy_detection()

    # 7. ì„±ëŠ¥ ì¸¡ì •
    tester.test_performance_comparison()

    print("\n" + "=" * 60)
    print("ğŸ“‹ KoELECTRA í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    print("âœ… í™•ì¸ëœ ê¸°ëŠ¥:")
    print("  - í•œêµ­ì–´ íŠ¹í™” í† í¬ë‚˜ì´ì§• ìš°ìˆ˜")
    print("  - íš¨ìœ¨ì ì¸ ì²˜ë¦¬ ì†ë„ (ELECTRA êµ¬ì¡°)")
    print("  - í•œêµ­ì–´ ë¬¸ë§¥ ì´í•´ ëŠ¥ë ¥")
    print("  - ë„ì–´ì“°ê¸°, ì¡´ëŒ“ë§ ë“± í•œêµ­ì–´ íŠ¹ì„± ì¸ì‹")
    print()
    print("ğŸ’ª KoELECTRA ê°•ì :")
    print("  - KoBERT ëŒ€ë¹„ ë¹ ë¥¸ ì¶”ë¡  ì†ë„")
    print("  - í•œêµ­ì–´ ë¬¸ë²• êµ¬ì¡° ì´í•´")
    print("  - íš¨ìœ¨ì ì¸ ë©”ëª¨ë¦¬ ì‚¬ìš©")
    print()
    print("âš ï¸ í•œê³„ì :")
    print("  - ê°œì¸ì •ë³´ íŠ¹í™” fine-tuning í•„ìš”")
    print("  - ì¡°í•© ìœ„í—˜ë„ íŒë‹¨ ì•Œê³ ë¦¬ì¦˜ ë³„ë„ êµ¬í˜„ í•„ìš”")
    print()
    print("ğŸ¯ ê²°ë¡ : KoELECTRAëŠ” ì†ë„ì™€ í•œêµ­ì–´ ì´í•´ì˜ ê· í˜•ì´ ìš°ìˆ˜í•˜ì—¬")
    print("       ì‹¤ì‹œê°„ ê°œì¸ì •ë³´ ê°ì§€ ì‹œìŠ¤í…œì— ì í•©í•œ í›„ë³´")

if __name__ == "__main__":
    main()
