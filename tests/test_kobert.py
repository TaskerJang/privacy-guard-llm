#!/usr/bin/env python3
"""
KoBERT ê°œì¸ì •ë³´ ë¬¸ë§¥ ì´í•´ í…ŒìŠ¤íŠ¸
"""

import torch
import time
import sys

def test_kobert_installation():
    """KoBERT ì„¤ì¹˜ í™•ì¸"""
    try:
        from kobert import get_pytorch_kobert_model, get_tokenizer_path
        from gluonnlp.data import SentencepieceTokenizer
        print("âœ… KoBERT ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì„±ê³µ!")
        return True
    except ImportError as e:
        print(f"âŒ KoBERT import ì‹¤íŒ¨: {e}")
        return False

def load_kobert_model():
    """KoBERT ëª¨ë¸ ë¡œë”©"""
    try:
        print("ğŸ”„ KoBERT ëª¨ë¸ ë¡œë”© ì¤‘...")
        start_time = time.time()

        from kobert import get_pytorch_kobert_model, get_tokenizer_path
        from gluonnlp.data import SentencepieceTokenizer

        model, vocab = get_pytorch_kobert_model()
        model.eval()

        tok_path = get_tokenizer_path()
        tokenizer = SentencepieceTokenizer(tok_path)

        load_time = time.time() - start_time
        print(f"âœ… KoBERT ëª¨ë¸ ë¡œë”© ì™„ë£Œ! (ì†Œìš”ì‹œê°„: {load_time:.2f}ì´ˆ)")

        return model, vocab, tokenizer
    except Exception as e:
        print(f"âŒ KoBERT ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return None, None, None

def test_tokenization(tokenizer):
    """í† í¬ë‚˜ì´ì§• í…ŒìŠ¤íŠ¸"""
    print("\nğŸ”¤ í† í¬ë‚˜ì´ì§• í…ŒìŠ¤íŠ¸")
    print("-" * 50)

    test_sentences = [
        "ì•ˆë…•í•˜ì„¸ìš” ê¹€ì² ìˆ˜ì…ë‹ˆë‹¤",
        "ì „í™”ë²ˆí˜¸ëŠ” 010-1234-5678ì…ë‹ˆë‹¤",
        "35ì„¸ ë‚¨ì„± ì˜ì‚¬ì´ê³  ê°•ë‚¨êµ¬ì— ê±°ì£¼í•©ë‹ˆë‹¤",
        "API í‚¤ëŠ” sk-abc123ì…ë‹ˆë‹¤"
    ]

    for sentence in test_sentences:
        tokens = tokenizer(sentence)
        print(f"ì›ë¬¸: {sentence}")
        print(f"í† í°: {tokens}")
        print()

def get_sentence_embedding(model, vocab, tokenizer, text):
    """ë¬¸ì¥ ì„ë² ë”© ì¶”ì¶œ"""
    tokens = tokenizer(text)
    tokens = ['[CLS]'] + tokens + ['[SEP]']

    # í† í°ì„ IDë¡œ ë³€í™˜
    token_ids = [vocab.to_indices(token) for token in tokens]

    # íŒ¨ë”© (ìµœëŒ€ 128ë¡œ ì œí•œ)
    max_length = 128
    if len(token_ids) > max_length:
        token_ids = token_ids[:max_length]
    else:
        token_ids += [vocab['[PAD]']] * (max_length - len(token_ids))

    # attention mask
    attention_mask = [1 if token_id != vocab['[PAD]'] else 0 for token_id in token_ids]
    token_type_ids = [0] * max_length

    # í…ì„œë¡œ ë³€í™˜
    input_ids = torch.LongTensor([token_ids])
    attention_mask = torch.LongTensor([attention_mask])
    token_type_ids = torch.LongTensor([token_type_ids])

    with torch.no_grad():
        sequence_output, pooled_output = model(input_ids, attention_mask, token_type_ids)

    return pooled_output.squeeze()

def test_similarity(model, vocab, tokenizer):
    """ë¬¸ë§¥ ìœ ì‚¬ë„ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ” ë¬¸ë§¥ ìœ ì‚¬ë„ í…ŒìŠ¤íŠ¸")
    print("-" * 50)

    test_pairs = [
        ("í™˜ì ê¹€ì² ìˆ˜ê°€ ìˆ˜ìˆ ë°›ì•˜ìŠµë‹ˆë‹¤", "ê¹€ì² ìˆ˜ êµìˆ˜ë‹˜ì´ ê°•ì˜í•˜ì…¨ìŠµë‹ˆë‹¤", "í™˜ì vs êµìˆ˜"),
        ("35ì„¸ ë‚¨ì„± ì˜ì‚¬", "40ëŒ€ ì—¬ì„± ê°„í˜¸ì‚¬", "ì˜ë£Œì§„ ì •ë³´"),
        ("API í‚¤ sk-abc123", "ë¹„ë°€ë²ˆí˜¸ password123", "ì¸ì¦ ì •ë³´"),
        ("í˜ˆë‹¹ 350 ì¼€í†¤ì‚°ì¦", "í˜ˆì•• 140/90 ê³ í˜ˆì••", "ì˜ë£Œ ìˆ˜ì¹˜")
    ]

    for text1, text2, desc in test_pairs:
        try:
            emb1 = get_sentence_embedding(model, vocab, tokenizer, text1)
            emb2 = get_sentence_embedding(model, vocab, tokenizer, text2)

            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
            similarity = torch.cosine_similarity(emb1, emb2, dim=0).item()

            print(f"ğŸ“Š {desc}")
            print(f"  í…ìŠ¤íŠ¸1: {text1}")
            print(f"  í…ìŠ¤íŠ¸2: {text2}")
            print(f"  ìœ ì‚¬ë„: {similarity:.4f}")

            if similarity > 0.8:
                print("  ğŸ”´ ë§¤ìš° ìœ ì‚¬")
            elif similarity > 0.6:
                print("  ğŸŸ¡ ì–´ëŠì •ë„ ìœ ì‚¬")
            elif similarity > 0.4:
                print("  ğŸŸ¢ ì•½ê°„ ìœ ì‚¬")
            else:
                print("  ğŸ”µ ë‚®ì€ ìœ ì‚¬ë„")
            print()

        except Exception as e:
            print(f"  âŒ ì˜¤ë¥˜: {e}")
            print()

def test_privacy_detection(model, vocab, tokenizer):
    """ê°œì¸ì •ë³´ ê°ì§€ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
    print("\nğŸ›¡ï¸ ê°œì¸ì •ë³´ ê°ì§€ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    print("-" * 50)

    test_cases = [
        ("ì•ˆë…•í•˜ì„¸ìš” ì œ ì´ë¦„ì€ ê¹€ì² ìˆ˜ì…ë‹ˆë‹¤", "ì§ì ‘ ê°œì¸ì •ë³´", "HIGH"),
        ("ì „í™”ë²ˆí˜¸ëŠ” 010-1234-5678ì…ë‹ˆë‹¤", "ì „í™”ë²ˆí˜¸", "HIGH"),
        ("35ì„¸ ë‚¨ì„± ì˜ì‚¬ì´ê³  ê°•ë‚¨êµ¬ì— ê±°ì£¼í•©ë‹ˆë‹¤", "ì¡°í•© ì •ë³´", "MEDIUM"),
        ("í™˜ì ê¹€ì² ìˆ˜ê°€ ì–´ì œ ìˆ˜ìˆ ë°›ì•˜ìŠµë‹ˆë‹¤", "ì˜ë£Œ ë§¥ë½", "HIGH"),
        ("ê¹€ì² ìˆ˜ êµìˆ˜ë‹˜ì´ ê°•ì˜í•˜ì…¨ìŠµë‹ˆë‹¤", "êµìœ¡ ë§¥ë½", "LOW"),
        ("API í‚¤ëŠ” sk-abc123ì…ë‹ˆë‹¤", "ê¸°ìˆ  ì •ë³´", "HIGH"),
        ("ì˜¤ëŠ˜ ë‚ ì”¨ê°€ ì¢‹ë„¤ìš”", "ì¼ë°˜ í…ìŠ¤íŠ¸", "NONE")
    ]

    for text, category, expected_risk in test_cases:
        try:
            embedding = get_sentence_embedding(model, vocab, tokenizer, text)

            # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± ìœ„í—˜ë„ ê³„ì‚° (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë¶„ë¥˜ê¸° í•„ìš”)
            privacy_score = 0.0

            # í‚¤ì›Œë“œ ê¸°ë°˜ ì ìˆ˜
            if any(keyword in text for keyword in ['ì´ë¦„', 'ê¹€ì² ìˆ˜', 'ë°•ì˜í¬']):
                privacy_score += 0.3
            if any(keyword in text for keyword in ['010-', 'ì „í™”ë²ˆí˜¸']):
                privacy_score += 0.4
            if any(keyword in text for keyword in ['ì˜ì‚¬', 'í™˜ì', 'ìˆ˜ìˆ ']):
                privacy_score += 0.2
            if any(keyword in text for keyword in ['API', 'sk-', 'password']):
                privacy_score += 0.4
            if any(keyword in text for keyword in ['ì„¸', 'ê°•ë‚¨êµ¬', 'ê±°ì£¼']):
                privacy_score += 0.1

            print(f"ğŸ“ {category}")
            print(f"  í…ìŠ¤íŠ¸: {text}")
            print(f"  ì˜ˆìƒ ìœ„í—˜ë„: {expected_risk}")
            print(f"  ê³„ì‚°ëœ ì ìˆ˜: {privacy_score:.2f}")
            print(f"  ì„ë² ë”© ì°¨ì›: {embedding.shape}")
            print()

        except Exception as e:
            print(f"  âŒ ì˜¤ë¥˜: {e}")
            print()

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸ§ª KoBERT ê°œì¸ì •ë³´ ê°ì§€ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)

    # 1. ì„¤ì¹˜ í™•ì¸
    if not test_kobert_installation():
        print("âŒ KoBERT ì„¤ì¹˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        sys.exit(1)

    # 2. ëª¨ë¸ ë¡œë”©
    model, vocab, tokenizer = load_kobert_model()
    if model is None:
        print("âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨")
        sys.exit(1)

    # 3. í† í¬ë‚˜ì´ì§• í…ŒìŠ¤íŠ¸
    test_tokenization(tokenizer)

    # 4. ìœ ì‚¬ë„ í…ŒìŠ¤íŠ¸
    test_similarity(model, vocab, tokenizer)

    # 5. ê°œì¸ì •ë³´ ê°ì§€ í…ŒìŠ¤íŠ¸
    test_privacy_detection(model, vocab, tokenizer)

    print("\n" + "=" * 60)
    print("ğŸ“‹ KoBERT í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)
    print("âœ… í™•ì¸ëœ ê¸°ëŠ¥:")
    print("  - í•œêµ­ì–´ í† í¬ë‚˜ì´ì§•")
    print("  - ë¬¸ì¥ ì„ë² ë”© ìƒì„±")
    print("  - ê¸°ë³¸ì ì¸ ë¬¸ë§¥ ì´í•´")
    print()
    print("â“ ì¶”ê°€ ê°œë°œ í•„ìš”:")
    print("  - ê°œì¸ì •ë³´ íŠ¹í™” ë¶„ë¥˜ê¸°")
    print("  - ì¡°í•© ìœ„í—˜ë„ ê³„ì‚° ì•Œê³ ë¦¬ì¦˜")
    print("  - ë„ë©”ì¸ë³„ ë¯¼ê°ì •ë³´ íŒ¨í„´")
    print()
    print("ğŸ¯ ê²°ë¡ : KoBERTëŠ” ê¸°ë³¸ ë¬¸ë§¥ ì´í•´ê°€ ê°€ëŠ¥í•˜ë©°,")
    print("       ê°œì¸ì •ë³´ ê°ì§€ìš© fine-tuningì„ í†µí•´ í™œìš© ê°€ëŠ¥")

if __name__ == "__main__":
    main()