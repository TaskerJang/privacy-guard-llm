"""
LLM API í´ë¼ì´ì–¸íŠ¸ - ë‹¤ì–‘í•œ LLM ì œê³µì í†µí•© í´ë¼ì´ì–¸íŠ¸
OpenAI, Anthropic, Google, Cohere API í˜¸ì¶œ ë° ê²°ê³¼ ì²˜ë¦¬
"""

import json
import time
import re
import logging
from typing import Dict, List, Tuple, Optional, Any
from dataclasses import dataclass
from datetime import datetime

from llm_configs import LLMConfig, RiskLevel

# ê° LLM ë¼ì´ë¸ŒëŸ¬ë¦¬ import
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

try:
    import google.generativeai as genai
    GOOGLE_AVAILABLE = True
except ImportError:
    GOOGLE_AVAILABLE = False

try:
    import cohere
    COHERE_AVAILABLE = True
except ImportError:
    COHERE_AVAILABLE = False

@dataclass
class APIResponse:
    """API ì‘ë‹µ ê²°ê³¼"""
    success: bool
    content: str
    token_count: int
    processing_time: float
    cost: float
    error_message: str = ""
    raw_response: Any = None

class LLMAPIClient:
    """LLM API í†µí•© í´ë¼ì´ì–¸íŠ¸"""

    def __init__(self, max_cost: float = 5.0):
        self.logger = logging.getLogger(__name__)
        self.total_cost = 0.0
        self.max_cost = max_cost
        self.request_count = 0

        # API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self._setup_clients()

    def _setup_clients(self):
        """API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        self.clients = {}

        # OpenAI í´ë¼ì´ì–¸íŠ¸
        if OPENAI_AVAILABLE:
            self.clients['openai'] = None  # ë‚˜ì¤‘ì— API í‚¤ì™€ í•¨ê»˜ ì´ˆê¸°í™”

        # Anthropic í´ë¼ì´ì–¸íŠ¸
        if ANTHROPIC_AVAILABLE:
            self.clients['anthropic'] = None

        # Google í´ë¼ì´ì–¸íŠ¸
        if GOOGLE_AVAILABLE:
            self.clients['google'] = None

        # Cohere í´ë¼ì´ì–¸íŠ¸
        if COHERE_AVAILABLE:
            self.clients['cohere'] = None

    def _get_client(self, provider: str, api_key: str):
        """ì œê³µìë³„ í´ë¼ì´ì–¸íŠ¸ ë°˜í™˜"""
        if provider == 'openai' and OPENAI_AVAILABLE:
            if self.clients['openai'] is None:
                self.clients['openai'] = OpenAI(api_key=api_key)
            return self.clients['openai']

        elif provider == 'anthropic' and ANTHROPIC_AVAILABLE:
            if self.clients['anthropic'] is None:
                self.clients['anthropic'] = anthropic.Anthropic(api_key=api_key)
            return self.clients['anthropic']

        elif provider == 'google' and GOOGLE_AVAILABLE:
            if self.clients['google'] is None:
                genai.configure(api_key=api_key)
            return genai

        elif provider == 'cohere' and COHERE_AVAILABLE:
            if self.clients['cohere'] is None:
                self.clients['cohere'] = cohere.Client(api_key=api_key)
            return self.clients['cohere']

        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì œê³µì: {provider}")

    def _call_openai_api(self, config: LLMConfig, prompt: str) -> APIResponse:
        """OpenAI API í˜¸ì¶œ"""
        start_time = time.time()

        try:
            client = self._get_client('openai', config.api_key)

            response = client.chat.completions.create(
                model=config.model_id,
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ê°œì¸ì •ë³´ ë³´í˜¸ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì •í™•í•œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                temperature=config.temperature,
                max_tokens=config.max_tokens
            )

            content = response.choices[0].message.content
            token_count = response.usage.total_tokens
            processing_time = time.time() - start_time
            cost = token_count * config.cost_per_1k_tokens / 1000

            return APIResponse(
                success=True,
                content=content,
                token_count=token_count,
                processing_time=processing_time,
                cost=cost,
                raw_response=response
            )

        except Exception as e:
            self.logger.error(f"OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return APIResponse(
                success=False,
                content="",
                token_count=0,
                processing_time=time.time() - start_time,
                cost=0.0,
                error_message=str(e)
            )

    def _call_anthropic_api(self, config: LLMConfig, prompt: str) -> APIResponse:
        """Anthropic API í˜¸ì¶œ"""
        start_time = time.time()

        try:
            client = self._get_client('anthropic', config.api_key)

            response = client.messages.create(
                model=config.model_id,
                max_tokens=config.max_tokens,
                temperature=config.temperature,
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )

            content = response.content[0].text
            token_count = response.usage.input_tokens + response.usage.output_tokens
            processing_time = time.time() - start_time
            cost = token_count * config.cost_per_1k_tokens / 1000

            return APIResponse(
                success=True,
                content=content,
                token_count=token_count,
                processing_time=processing_time,
                cost=cost,
                raw_response=response
            )

        except Exception as e:
            self.logger.error(f"Anthropic API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return APIResponse(
                success=False,
                content="",
                token_count=0,
                processing_time=time.time() - start_time,
                cost=0.0,
                error_message=str(e)
            )

    def _call_google_api(self, config: LLMConfig, prompt: str) -> APIResponse:
        """Google API í˜¸ì¶œ"""
        start_time = time.time()

        try:
            genai_client = self._get_client('google', config.api_key)
            model = genai_client.GenerativeModel(config.model_id)

            response = model.generate_content(
                prompt,
                generation_config=genai_client.types.GenerationConfig(
                    temperature=config.temperature,
                    max_output_tokens=config.max_tokens,
                )
            )

            content = response.text
            # Googleì€ ì •í™•í•œ í† í° ì¹´ìš´íŠ¸ë¥¼ ì œê³µí•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ
            token_count = response.usage_metadata.total_token_count if hasattr(response, 'usage_metadata') else len(content.split()) * 1.3
            processing_time = time.time() - start_time
            cost = token_count * config.cost_per_1k_tokens / 1000

            return APIResponse(
                success=True,
                content=content,
                token_count=int(token_count),
                processing_time=processing_time,
                cost=cost,
                raw_response=response
            )

        except Exception as e:
            self.logger.error(f"Google API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return APIResponse(
                success=False,
                content="",
                token_count=0,
                processing_time=time.time() - start_time,
                cost=0.0,
                error_message=str(e)
            )

    def _call_cohere_api(self, config: LLMConfig, prompt: str) -> APIResponse:
        """Cohere API í˜¸ì¶œ"""
        start_time = time.time()

        try:
            client = self._get_client('cohere', config.api_key)

            response = client.chat(
                model=config.model_id,
                message=prompt,
                temperature=config.temperature,
                max_tokens=config.max_tokens
            )

            content = response.text
            token_count = response.meta.tokens.input_tokens + response.meta.tokens.output_tokens if hasattr(response, 'meta') else len(content.split()) * 1.3
            processing_time = time.time() - start_time
            cost = token_count * config.cost_per_1k_tokens / 1000

            return APIResponse(
                success=True,
                content=content,
                token_count=int(token_count),
                processing_time=processing_time,
                cost=cost,
                raw_response=response
            )

        except Exception as e:
            self.logger.error(f"Cohere API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            return APIResponse(
                success=False,
                content="",
                token_count=0,
                processing_time=time.time() - start_time,
                cost=0.0,
                error_message=str(e)
            )

    def call_api(self, config: LLMConfig, prompt: str) -> APIResponse:
        """í†µí•© API í˜¸ì¶œ"""
        # ë¹„ìš© ì²´í¬
        estimated_cost = len(prompt.split()) * 1.3 * config.cost_per_1k_tokens / 1000
        if self.total_cost + estimated_cost > self.max_cost:
            return APIResponse(
                success=False,
                content="",
                token_count=0,
                processing_time=0.0,
                cost=0.0,
                error_message=f"ë¹„ìš© í•œë„ ì´ˆê³¼: {self.total_cost + estimated_cost:.4f} > {self.max_cost:.4f}"
            )

        # ì œê³µìë³„ API í˜¸ì¶œ
        if config.provider == 'openai':
            response = self._call_openai_api(config, prompt)
        elif config.provider == 'anthropic':
            response = self._call_anthropic_api(config, prompt)
        elif config.provider == 'google':
            response = self._call_google_api(config, prompt)
        elif config.provider == 'cohere':
            response = self._call_cohere_api(config, prompt)
        else:
            return APIResponse(
                success=False,
                content="",
                token_count=0,
                processing_time=0.0,
                cost=0.0,
                error_message=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì œê³µì: {config.provider}"
            )

        # ë¹„ìš© ì—…ë°ì´íŠ¸
        if response.success:
            self.total_cost += response.cost
            self.request_count += 1

        return response

    def parse_response(self, response_content: str) -> Dict[str, Any]:
        """API ì‘ë‹µ íŒŒì‹±"""
        try:
            # JSON ë¶€ë¶„ ì¶”ì¶œ
            json_match = re.search(r'\{.*\}', response_content, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                result = json.loads(json_str)

                # í•„ìˆ˜ í•„ë“œ í™•ì¸
                required_fields = ['risk_score', 'risk_level', 'detected_entities', 'explanation', 'domain']
                for field in required_fields:
                    if field not in result:
                        result[field] = self._get_default_value(field)

                return result
            else:
                # JSONì´ ì—†ëŠ” ê²½ìš° í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ ì‹œë„
                return self._extract_from_text(response_content)

        except json.JSONDecodeError as e:
            self.logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            return self._extract_from_text(response_content)

    def _get_default_value(self, field: str) -> Any:
        """ê¸°ë³¸ê°’ ë°˜í™˜"""
        defaults = {
            'risk_score': 0,
            'risk_level': 'NONE',
            'detected_entities': [],
            'explanation': 'JSON íŒŒì‹± ì‹¤íŒ¨',
            'domain': 'unknown',
            'recommendations': []
        }
        return defaults.get(field, None)

    def _extract_from_text(self, text: str) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ ì¶”ì¶œ (JSON íŒŒì‹± ì‹¤íŒ¨ì‹œ)"""
        result = {
            'risk_score': 0,
            'risk_level': 'NONE',
            'detected_entities': [],
            'explanation': text[:200] + "..." if len(text) > 200 else text,
            'domain': 'unknown',
            'recommendations': []
        }

        # ìœ„í—˜ë„ í‚¤ì›Œë“œ ê²€ìƒ‰
        text_lower = text.lower()
        if 'critical' in text_lower or 'very high' in text_lower:
            result['risk_level'] = 'CRITICAL'
            result['risk_score'] = 95
        elif 'high' in text_lower:
            result['risk_level'] = 'HIGH'
            result['risk_score'] = 80
        elif 'medium' in text_lower or 'moderate' in text_lower:
            result['risk_level'] = 'MEDIUM'
            result['risk_score'] = 60
        elif 'low' in text_lower:
            result['risk_level'] = 'LOW'
            result['risk_score'] = 30

        return result

    def analyze_text(self, text: str, config: LLMConfig, prompt: str, expected_risk: str) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ ë¶„ì„ (í†µí•© ì¸í„°í˜ì´ìŠ¤)"""
        # API í˜¸ì¶œ
        api_response = self.call_api(config, prompt)

        if not api_response.success:
            return {
                'text': text,
                'model_name': config.name,
                'predicted_risk': 'NONE',
                'expected_risk': expected_risk,
                'risk_score': 0.0,
                'detected_entities': [],
                'explanation': api_response.error_message,
                'domain': 'unknown',
                'processing_time': api_response.processing_time,
                'token_count': api_response.token_count,
                'cost': api_response.cost,
                'correct': False,
                'error': True,
                'recommendations': []
            }

        # ì‘ë‹µ íŒŒì‹±
        parsed_result = self.parse_response(api_response.content)

        # ìœ„í—˜ë„ ë ˆë²¨ ë§¤í•‘
        risk_level_map = {
            'CRITICAL': RiskLevel.CRITICAL,
            'HIGH': RiskLevel.HIGH,
            'MEDIUM': RiskLevel.MEDIUM,
            'LOW': RiskLevel.LOW,
            'NONE': RiskLevel.NONE
        }

        predicted_risk = parsed_result['risk_level']
        is_correct = predicted_risk == expected_risk

        return {
            'text': text,
            'model_name': config.name,
            'predicted_risk': predicted_risk,
            'expected_risk': expected_risk,
            'risk_score': parsed_result['risk_score'] / 100.0,
            'detected_entities': parsed_result['detected_entities'],
            'explanation': parsed_result['explanation'],
            'domain': parsed_result['domain'],
            'processing_time': api_response.processing_time,
            'token_count': api_response.token_count,
            'cost': api_response.cost,
            'correct': is_correct,
            'error': False,
            'recommendations': parsed_result.get('recommendations', []),
            'raw_response': api_response.content
        }

    def get_cost_summary(self) -> Dict[str, Any]:
        """ë¹„ìš© ìš”ì•½ ì •ë³´"""
        return {
            'total_cost': self.total_cost,
            'max_cost': self.max_cost,
            'remaining_budget': self.max_cost - self.total_cost,
            'request_count': self.request_count,
            'average_cost_per_request': self.total_cost / self.request_count if self.request_count > 0 else 0
        }

    def reset_cost_tracking(self):
        """ë¹„ìš© ì¶”ì  ë¦¬ì…‹"""
        self.total_cost = 0.0
        self.request_count = 0
        self.logger.info("ë¹„ìš© ì¶”ì ì´ ë¦¬ì…‹ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    # API í´ë¼ì´ì–¸íŠ¸ í…ŒìŠ¤íŠ¸
    from llm_configs import LLMConfigManager

    config_manager = LLMConfigManager()
    api_client = LLMAPIClient(max_cost=1.0)

    # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ í™•ì¸
    available_models = config_manager.get_available_models()
    if available_models:
        print(f"ğŸ¤– ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {available_models}")

        # ì²« ë²ˆì§¸ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸
        model_name = available_models[0]
        config = config_manager.get_config(model_name)

        test_prompt = """
ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ ê°œì¸ì •ë³´ ìœ„í—˜ë„ë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:
"ê¹€ì² ìˆ˜(35ì„¸ ë‚¨ì„±)ê°€ ë‹¹ë‡¨ë³‘ ì§„ë‹¨ì„ ë°›ì•˜ìŠµë‹ˆë‹¤."

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
{
    "risk_score": 75,
    "risk_level": "HIGH",
    "detected_entities": ["ì´ë¦„", "ë‚˜ì´", "ì„±ë³„", "ì§ˆë³‘"],
    "explanation": "ê°œì¸ì •ë³´ì™€ ì˜ë£Œì •ë³´ ì¡°í•©",
    "domain": "medical"
}
"""

        print(f"\nğŸ§ª {model_name} í…ŒìŠ¤íŠ¸ ì¤‘...")
        result = api_client.analyze_text(
            text="ê¹€ì² ìˆ˜(35ì„¸ ë‚¨ì„±)ê°€ ë‹¹ë‡¨ë³‘ ì§„ë‹¨ì„ ë°›ì•˜ìŠµë‹ˆë‹¤.",
            config=config,
            prompt=test_prompt,
            expected_risk="HIGH"
        )

        print(f"âœ… ê²°ê³¼: {result}")
        print(f"ğŸ’° ë¹„ìš©: ${result['cost']:.4f}")

    else:
        print("âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")