import os
import torch
import numpy as np
import pandas as pd
from typing import List, Dict, Tuple, Any
from dataclasses import dataclass
import warnings
warnings.filterwarnings("ignore")

# í•™ìŠµëœ ëª¨ë¸ ë¡œë“œìš©
from transformers import AutoTokenizer, AutoModelForTokenClassification
from peft import PeftModel

# Copula ëª¨ë¸ìš© (2ë‹¨ê³„)
from copulas.multivariate import GaussianMultivariate

# ================== ë°ì´í„° êµ¬ì¡° ì •ì˜ ==================
@dataclass
class NERResult:
    """1ë‹¨ê³„ NER ê²°ê³¼"""
    token: str
    entity: str
    start_pos: int = 0
    end_pos: int = 0

@dataclass
class RiskWeight:
    """2ë‹¨ê³„ ìœ„í—˜ë„ ê°€ì¤‘ì¹˜"""
    token: str
    entity: str
    category: str  # 'ì§ì ‘', 'ê°„ì ‘', 'ê¸°íƒ€'
    risk_weight: int  # 0-100
    copula_feature: str = None

@dataclass
class MaskingResult:
    """ìµœì¢… ë§ˆìŠ¤í‚¹ ê²°ê³¼"""
    original_text: str
    masked_text: str
    masking_log: List[Dict]
    total_entities: int
    masked_entities: int

# ================== 1ë‹¨ê³„: í•™ìŠµëœ NER ëª¨ë¸ ==================
class TrainedNERModel:
    """í•™ìŠµëœ KoELECTRA NER ëª¨ë¸ ë¡œë”"""

    def __init__(self, model_path: str, base_model: str = "monologg/koelectra-base-v3-discriminator"):
        self.model_path = model_path
        self.base_model = base_model
        self.tokenizer = None
        self.model = None
        self.id2label = None
        self._load_model()

    def _load_model(self):
        """í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ"""
        try:
            # LoRA ì–´ëŒ‘í„°ì¸ì§€ ë³‘í•© ëª¨ë¸ì¸ì§€ í™•ì¸
            is_lora = os.path.exists(os.path.join(self.model_path, "adapter_config.json"))

            if is_lora:
                print(f"ğŸ”„ LoRA ì–´ëŒ‘í„° ë¡œë“œ ì¤‘: {self.model_path}")
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
                base = AutoModelForTokenClassification.from_pretrained(self.base_model)
                self.model = PeftModel.from_pretrained(base, self.model_path)
            else:
                print(f"ğŸ”„ ë³‘í•© ëª¨ë¸ ë¡œë“œ ì¤‘: {self.model_path}")
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
                self.model = AutoModelForTokenClassification.from_pretrained(self.model_path)

            self.model.eval()
            self.id2label = {int(k): v for k, v in self.model.config.id2label.items()}
            print(f"âœ… NER ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! ë¼ë²¨ ìˆ˜: {len(self.id2label)}")

        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ğŸ’¡ ë”ë¯¸ ëª¨ë¸ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤...")
            self._create_dummy_model()

    def _create_dummy_model(self):
        """í•™ìŠµëœ ëª¨ë¸ì´ ì—†ì„ ë•Œ ë”ë¯¸ ëª¨ë¸ ìƒì„±"""
        self.id2label = {
            0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG',
            5: 'B-LOC', 6: 'I-LOC', 7: 'B-DATE', 8: 'I-DATE',
            9: 'B-DISEASE', 10: 'I-DISEASE', 11: 'B-CONTACT', 12: 'I-CONTACT'
        }
        print("âš ï¸  ë”ë¯¸ NER ëª¨ë¸ ì‚¬ìš© ì¤‘ (ì‹¤ì œ ëª¨ë¸ ê²½ë¡œë¥¼ ì„¤ì •í•˜ì„¸ìš”)")

    def predict(self, sentence: str) -> List[NERResult]:
        """ë¬¸ì¥ì—ì„œ ê°œì²´ëª… ì¶”ì¶œ"""
        if self.model is None:
            return self._dummy_predict(sentence)

        tokens = sentence.split()
        enc = self.tokenizer(
            tokens, is_split_into_words=True,
            return_tensors="pt", padding="max_length",
            truncation=True, max_length=128
        )

        with torch.no_grad():
            logits = self.model(**enc).logits

        preds = logits.argmax(-1)[0].tolist()
        word_ids = enc.word_ids()

        results = []
        last_word_id = None

        for i, word_id in enumerate(word_ids):
            if word_id is None or word_id == last_word_id:
                continue

            entity_label = self.id2label[preds[i]]
            results.append(NERResult(
                token=tokens[word_id],
                entity=entity_label,
                start_pos=0,  # ì‹¤ì œ êµ¬í˜„ì‹œ ê³„ì‚°
                end_pos=len(tokens[word_id])
            ))
            last_word_id = word_id

        return results

    def _dummy_predict(self, sentence: str) -> List[NERResult]:
        """ë”ë¯¸ ì˜ˆì¸¡ (ëª¨ë¸ ì—†ì„ ë•Œ)"""
        # ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ë”ë¯¸ NER
        tokens = sentence.split()
        results = []

        for token in tokens:
            entity = 'O'
            # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±
            if any(name in token for name in ['ê¹€', 'ë°•', 'ì´', 'ìµœ', 'ì •', 'í•œ']):
                entity = 'B-PER'
            elif any(org in token for org in ['ë³‘ì›', 'ì˜ë£Œì›', 'ì„¼í„°']):
                entity = 'B-ORG'
            elif any(date in token for date in ['ë…„', 'ì›”', 'ì¼']):
                entity = 'B-DATE'
            elif '010-' in token or '02-' in token:
                entity = 'B-CONTACT'

            results.append(NERResult(token=token, entity=entity))

        return results

# ================== 2ë‹¨ê³„: Copula ìœ„í—˜ë„ ë¶„ì„ ==================
class CopulaRiskAnalyzer:
    """Copula ê¸°ë°˜ ìœ„í—˜ë„ ë¶„ì„ê¸°"""

    def __init__(self):
        self._setup_copula_model()
        self.token_to_feature = {
            'ì„œìš¸ëŒ€ë³‘ì›': {'ê¸°ê´€_ì„œìš¸ëŒ€ë³‘ì›': 1, 'ê¸°ê´€_ì‚¼ì„±ì„œìš¸': 0, 'ê¸°ê´€_ì—°ì„¸ì˜ë£Œì›': 0},
            'ì‚¼ì„±ì„œìš¸ë³‘ì›': {'ê¸°ê´€_ì„œìš¸ëŒ€ë³‘ì›': 0, 'ê¸°ê´€_ì‚¼ì„±ì„œìš¸': 1, 'ê¸°ê´€_ì—°ì„¸ì˜ë£Œì›': 0},
            'ê°„ì•”': {'ì§ˆë³‘_ê°„ì•”': 1, 'ì§ˆë³‘_ë°±í˜ˆë³‘': 0, 'ì§ˆë³‘_ê³ í˜ˆì••': 0},
            'ë°±í˜ˆë³‘': {'ì§ˆë³‘_ê°„ì•”': 0, 'ì§ˆë³‘_ë°±í˜ˆë³‘': 1, 'ì§ˆë³‘_ê³ í˜ˆì••': 0},
            '2023ë…„': {'ë‚ ì§œ_2023ë…„': 1, 'ë‚ ì§œ_2022ë…„': 0, 'ë‚ ì§œ_2021ë…„': 0},
            '2024ë…„': {'ë‚ ì§œ_2023ë…„': 0, 'ë‚ ì§œ_2022ë…„': 1, 'ë‚ ì§œ_2021ë…„': 0},
        }

    def _setup_copula_model(self):
        """Copula ëª¨ë¸ í•™ìŠµ"""
        np.random.seed(42)
        df_sample = pd.DataFrame({
            'ê¸°ê´€': np.random.choice(['ì„œìš¸ëŒ€ë³‘ì›', 'ì‚¼ì„±ì„œìš¸', 'ì—°ì„¸ì˜ë£Œì›'], 1000),
            'ì§ˆë³‘': np.random.choice(['ê°„ì•”', 'ë°±í˜ˆë³‘', 'ê³ í˜ˆì••'], 1000),
            'ë‚ ì§œ': np.random.choice(['2023ë…„', '2022ë…„', '2021ë…„'], 1000)
        })

        df_encoded = pd.get_dummies(df_sample).astype(float)
        self.copula_model = GaussianMultivariate()
        self.copula_model.fit(df_encoded)
        self.samples = self.copula_model.sample(10000).round()

    def calculate_risk_weights(self, ner_results: List[NERResult]) -> List[RiskWeight]:
        """NER ê²°ê³¼ë¥¼ ìœ„í—˜ë„ë¡œ ë³€í™˜"""
        risk_weights = []

        for ner_result in ner_results:
            category = self._categorize_entity(ner_result.entity)
            risk_weight = self._calculate_single_risk(ner_result.token, ner_result.entity, category)

            copula_feature = None
            if ner_result.token in self.token_to_feature:
                copula_feature = ", ".join(self.token_to_feature[ner_result.token].keys())

            risk_weights.append(RiskWeight(
                token=ner_result.token,
                entity=ner_result.entity,
                category=category,
                risk_weight=risk_weight,
                copula_feature=copula_feature
            ))

        return risk_weights

    def _categorize_entity(self, entity: str) -> str:
        """ì—”í‹°í‹°ë¥¼ ì§ì ‘/ê°„ì ‘/ê¸°íƒ€ë¡œ ë¶„ë¥˜"""
        direct_entities = ['B-PER', 'I-PER', 'B-CONTACT', 'I-CONTACT']
        indirect_entities = ['B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-DATE', 'I-DATE', 'B-DISEASE', 'I-DISEASE']

        if entity in direct_entities:
            return 'ì§ì ‘'
        elif entity in indirect_entities:
            return 'ê°„ì ‘'
        else:
            return 'ê¸°íƒ€'

    def _calculate_single_risk(self, token: str, entity: str, category: str) -> int:
        """ë‹¨ì¼ í† í°ì˜ ìœ„í—˜ë„ ê³„ì‚°"""
        if category == 'ì§ì ‘':
            return 100
        elif category == 'ê°„ì ‘':
            feature_info = self.token_to_feature.get(token)
            if feature_info:
                risk_score = self._calculate_copula_risk(feature_info)
                return round(risk_score * 100)
            return 30  # ê¸°ë³¸ ê°„ì ‘ ìœ„í—˜ë„
        else:
            return 0

    def _calculate_copula_risk(self, feature_dict: Dict) -> float:
        """Copula ê¸°ë°˜ ìœ„í—˜ë„ ê³„ì‚°"""
        match = (self.samples[list(feature_dict.keys())] == pd.Series(feature_dict)).all(axis=1)
        probability = match.sum() / len(self.samples)
        return 1 - probability  # í¬ê·€í• ìˆ˜ë¡ ìœ„í—˜

# ================== 3ë‹¨ê³„: ë¬¸ë§¥ì  ìœ„í—˜ ë¶„ì„ ==================
class ContextualRiskAnalyzer:
    """ë¬¸ë§¥ì„ ê³ ë ¤í•œ ìœ„í—˜ë„ ì¡°ì •"""

    def __init__(self):
        self.high_risk_combinations = {
            ('PER', 'ORG', 'DATE'): 1.5,
            ('PER', 'DISEASE', 'ORG'): 1.8,
            ('PER', 'CONTACT'): 2.0,
            ('ORG', 'DATE', 'DISEASE'): 1.3,
        }

        self.medical_risk_keywords = {
            'ì§„ë‹¨': 1.2, 'ìˆ˜ìˆ ': 1.2, 'ì…ì›': 1.2, 'ì¹˜ë£Œ': 1.2,
            'ì•”': 1.3, 'ì¢…ì–‘': 1.3, 'ì§ˆí™˜': 1.3,
            'ì‘ê¸‰': 1.5, 'ì¤‘í™˜ì': 1.5
        }

    def analyze_contextual_risk(self, text: str, risk_weights: List[RiskWeight]) -> List[RiskWeight]:
        """ë¬¸ë§¥ì„ ê³ ë ¤í•œ ìœ„í—˜ë„ ì¬ì¡°ì •"""
        entity_types = [self._normalize_entity_type(rw.entity) for rw in risk_weights if rw.entity != 'O']

        combination_multiplier = self._get_combination_multiplier(entity_types)
        keyword_multiplier = self._get_keyword_multiplier(text)

        adjusted_weights = []
        for risk_weight in risk_weights:
            adjusted_weight = risk_weight.risk_weight

            if adjusted_weight > 0:
                adjusted_weight = min(100, int(adjusted_weight * combination_multiplier * keyword_multiplier))

            adjusted_weights.append(RiskWeight(
                token=risk_weight.token,
                entity=risk_weight.entity,
                category=risk_weight.category,
                risk_weight=adjusted_weight,
                copula_feature=risk_weight.copula_feature
            ))

        return adjusted_weights

    def _normalize_entity_type(self, entity: str) -> str:
        if entity.startswith(('B-', 'I-')):
            return entity[2:]
        return entity

    def _get_combination_multiplier(self, entity_types: List[str]) -> float:
        entity_set = set(entity_types)
        max_multiplier = 1.0

        for pattern, multiplier in self.high_risk_combinations.items():
            if set(pattern).issubset(entity_set):
                max_multiplier = max(max_multiplier, multiplier)

        return max_multiplier

    def _get_keyword_multiplier(self, text: str) -> float:
        max_multiplier = 1.0
        for keyword, multiplier in self.medical_risk_keywords.items():
            if keyword in text:
                max_multiplier = max(max_multiplier, multiplier)
        return max_multiplier

# ================== 4ë‹¨ê³„: ë§ˆìŠ¤í‚¹ ì‹¤í–‰ ==================
class MaskingExecutor:
    """ë§ˆìŠ¤í‚¹ ì‹¤í–‰ê¸°"""

    def __init__(self, threshold: int = 50):
        self.threshold = threshold
        self.mask_patterns = {
            'PER': '[PERSON]',
            'ORG': '[HOSPITAL]',
            'LOC': '[LOCATION]',
            'DATE': '[DATE]',
            'DISEASE': '[DISEASE]',
            'CONTACT': '[CONTACT]',
            'CVL': '[TITLE]',
            'NUM': '[NUMBER]',
            'default': '[MASKED]'
        }

    def execute_masking(self, text: str, risk_weights: List[RiskWeight]) -> MaskingResult:
        """ì„ê³„ê°’ ê¸°ë°˜ ë§ˆìŠ¤í‚¹ ì‹¤í–‰"""
        masked_text = text
        masking_log = []
        masked_count = 0
        total_entities = len([rw for rw in risk_weights if rw.entity != 'O'])

        # ìœ„í—˜ë„ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_weights = sorted(risk_weights, key=lambda x: x.risk_weight, reverse=True)

        for risk_weight in sorted_weights:
            if risk_weight.risk_weight >= self.threshold and risk_weight.entity != 'O':
                entity_type = self._normalize_entity_type(risk_weight.entity)
                mask_pattern = self.mask_patterns.get(entity_type, self.mask_patterns['default'])

                if risk_weight.token in masked_text:
                    masked_text = masked_text.replace(risk_weight.token, mask_pattern, 1)
                    masked_count += 1

                    masking_log.append({
                        'token': risk_weight.token,
                        'entity': risk_weight.entity,
                        'risk_weight': risk_weight.risk_weight,
                        'masked_as': mask_pattern,
                        'reason': f'ìœ„í—˜ë„ {risk_weight.risk_weight} >= ì„ê³„ê°’ {self.threshold}'
                    })

        return MaskingResult(
            original_text=text,
            masked_text=masked_text,
            masking_log=masking_log,
            total_entities=total_entities,
            masked_entities=masked_count
        )

    def _normalize_entity_type(self, entity: str) -> str:
        if entity.startswith(('B-', 'I-')):
            return entity[2:]
        return entity

# ================== ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•© ==================
class CompleteMedicalDeidentificationPipeline:
    """ì™„ì „í•œ ì˜ë£Œ í…ìŠ¤íŠ¸ ë¹„ì‹ë³„í™” íŒŒì´í”„ë¼ì¸"""

    def __init__(self,
                 model_path: str = None,
                 threshold: int = 50,
                 use_contextual_analysis: bool = True):

        print("ğŸš€ ì˜ë£Œ í…ìŠ¤íŠ¸ ë¹„ì‹ë³„í™” íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì¤‘...")

        # 1ë‹¨ê³„: í•™ìŠµëœ NER ëª¨ë¸ ë¡œë“œ
        self.ner_model = TrainedNERModel(model_path) if model_path else TrainedNERModel("dummy")

        # 2ë‹¨ê³„: Copula ìœ„í—˜ë„ ë¶„ì„ê¸°
        self.copula_analyzer = CopulaRiskAnalyzer()

        # 3ë‹¨ê³„: ë¬¸ë§¥ì  ìœ„í—˜ ë¶„ì„ê¸°
        self.contextual_analyzer = ContextualRiskAnalyzer() if use_contextual_analysis else None

        # 4ë‹¨ê³„: ë§ˆìŠ¤í‚¹ ì‹¤í–‰ê¸°
        self.masking_executor = MaskingExecutor(threshold)

        print("âœ… íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ!")

    def process(self, text: str, verbose: bool = True) -> MaskingResult:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        if verbose:
            print(f"\nğŸ“ ì²˜ë¦¬í•  í…ìŠ¤íŠ¸: {text}")

        # 1ë‹¨ê³„: NER (í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©!)
        ner_results = self.ner_model.predict(text)
        if verbose:
            print(f"ğŸ” 1ë‹¨ê³„ NER ê²°ê³¼: {[(r.token, r.entity) for r in ner_results]}")

        # 2ë‹¨ê³„: Copula ìœ„í—˜ë„ ê³„ì‚°
        risk_weights = self.copula_analyzer.calculate_risk_weights(ner_results)
        if verbose:
            print(f"ğŸ“Š 2ë‹¨ê³„ ìœ„í—˜ë„: {[(r.token, r.risk_weight) for r in risk_weights if r.risk_weight > 0]}")

        # 3ë‹¨ê³„: ë¬¸ë§¥ì  ìœ„í—˜ë„ ì¡°ì •
        if self.contextual_analyzer:
            risk_weights = self.contextual_analyzer.analyze_contextual_risk(text, risk_weights)
            if verbose:
                print(f"ğŸ”„ 3ë‹¨ê³„ ì¡°ì •ëœ ìœ„í—˜ë„: {[(r.token, r.risk_weight) for r in risk_weights if r.risk_weight > 0]}")

        # 4ë‹¨ê³„: ë§ˆìŠ¤í‚¹ ì‹¤í–‰
        result = self.masking_executor.execute_masking(text, risk_weights)
        if verbose:
            print(f"ğŸ­ 4ë‹¨ê³„ ë§ˆìŠ¤í‚¹ ê²°ê³¼: {result.masked_text}")

        return result

    def print_detailed_analysis(self, result: MaskingResult):
        """ìƒì„¸ ë¶„ì„ ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ğŸ¥ ì˜ë£Œ í…ìŠ¤íŠ¸ ë¹„ì‹ë³„í™” ë¶„ì„ ê²°ê³¼")
        print("="*80)
        print(f"ğŸ“ ì›ë¬¸: {result.original_text}")
        print(f"ğŸ­ ë§ˆìŠ¤í‚¹: {result.masked_text}")
        print(f"ğŸ“Š í†µê³„: {result.masked_entities}/{result.total_entities} ê°œì²´ ë§ˆìŠ¤í‚¹")

        if result.masking_log:
            print("\nğŸ“‹ ë§ˆìŠ¤í‚¹ ìƒì„¸ ë¡œê·¸:")
            for i, log in enumerate(result.masking_log, 1):
                print(f"  {i}. '{log['token']}' â†’ {log['masked_as']} (ìœ„í—˜ë„: {log['risk_weight']})")
        else:
            print("\nâœ… ë§ˆìŠ¤í‚¹ì´ í•„ìš”í•œ ê³ ìœ„í—˜ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ================== í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ==================
def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸš€ ì™„ì „í•œ ì˜ë£Œ í…ìŠ¤íŠ¸ ë¹„ì‹ë³„í™” íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸")

    # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” (ì‹¤ì œ ëª¨ë¸ ê²½ë¡œ ì„¤ì • í•„ìš”)
    model_path = "ner-koelectra-lora-merged"  # ì‹¤ì œ í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œë¡œ ë³€ê²½
    pipeline = CompleteMedicalDeidentificationPipeline(
        model_path=model_path,  # ì—¬ê¸°ì— ì‹¤ì œ í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ ì…ë ¥!
        threshold=50,
        use_contextual_analysis=True
    )

    # í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
    test_cases = [
        "ê¹€ì² ìˆ˜ì”¨ê°€ 2023ë…„ 10ì›”ì— ì„œìš¸ëŒ€ë³‘ì›ì—ì„œ ê°„ì•” ì§„ë‹¨ì„ ë°›ì•˜ìŠµë‹ˆë‹¤.",
        "ë°•ì˜í¬(010-1234-5678)ëŠ” ì‚¼ì„±ì„œìš¸ë³‘ì›ì—ì„œ ìˆ˜ìˆ ì„ ë°›ì•˜ë‹¤.",
        "í™˜ìëŠ” ë‚´ì¼ ê²€ì‚¬ë¥¼ ë°›ì„ ì˜ˆì •ì…ë‹ˆë‹¤.",
        "ì´ìˆœì‹  êµìˆ˜ëŠ” ì—°ì„¸ì˜ë£Œì›ì—ì„œ ë°±í˜ˆë³‘ ì—°êµ¬ë¥¼ í•˜ê³  ìˆë‹¤."
    ]

    # ê° í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰
    for i, text in enumerate(test_cases, 1):
        print(f"\n{'='*20} í…ŒìŠ¤íŠ¸ {i} {'='*20}")
        result = pipeline.process(text, verbose=True)
        pipeline.print_detailed_analysis(result)

    print(f"\n{'='*80}")
    print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("ğŸ’¡ ì‹¤ì œ í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë ¤ë©´ model_pathë¥¼ ìˆ˜ì •í•˜ì„¸ìš”.")
    print("ğŸ’¡ ì˜ˆ: 'ner-koelectra-lora-merged' ë˜ëŠ” './results/(epoch-10, lr-8e-05, batch-8)ner-koelectra-lora-merged'")

if __name__ == "__main__":
    main()