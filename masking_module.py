import os
import torch
import numpy as np
import pandas as pd
from typing import List, Dict
from dataclasses import dataclass
import warnings
warnings.filterwarnings("ignore")

# í•™ìŠµëœ ëª¨ë¸ ë¡œë“œìš©
from transformers import AutoTokenizer, AutoModelForTokenClassification
from peft import PeftModel

# Copula ëª¨ë¸ìš© (2ë‹¨ê³„)
from copulas.multivariate import GaussianMultivariate

# ================== ë°ì´í„° êµ¬ì¡° ì •ì˜ ==================
@dataclass
class NERResult:
    """1ë‹¨ê³„ NER ê²°ê³¼"""
    token: str
    entity: str
    start_pos: int = 0
    end_pos: int = 0

@dataclass
class RiskWeight:
    """2ë‹¨ê³„ ìœ„í—˜ë„ ê°€ì¤‘ì¹˜"""
    token: str
    entity: str
    category: str  # 'ì§ì ‘', 'ê°„ì ‘', 'ê¸°íƒ€'
    risk_weight: int  # 0-100
    copula_feature: str = None

@dataclass
class MaskingResult:
    """ìµœì¢… ë§ˆìŠ¤í‚¹ ê²°ê³¼"""
    original_text: str
    masked_text: str
    masking_log: List[Dict]
    total_entities: int
    masked_entities: int

# ================== 1ë‹¨ê³„: í•™ìŠµëœ NER ëª¨ë¸ ==================
class TrainedNERModel:
    """í•™ìŠµëœ KoELECTRA NER ëª¨ë¸ ë¡œë”"""

    def __init__(self, model_path: str, base_model: str = "monologg/koelectra-base-v3-discriminator"):
        self.model_path = model_path
        self.base_model = base_model
        self.tokenizer = None
        self.model = None
        self.id2label = None
        self._load_model()

    def _load_model(self):
        try:
            is_lora = os.path.exists(os.path.join(self.model_path, "adapter_config.json"))
            if is_lora:
                print(f"ğŸ”„ LoRA ì–´ëŒ‘í„° ë¡œë“œ ì¤‘: {self.model_path}")
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
                base = AutoModelForTokenClassification.from_pretrained(self.base_model)
                self.model = PeftModel.from_pretrained(base, self.model_path)
            else:
                print(f"ğŸ”„ ë³‘í•© ëª¨ë¸ ë¡œë“œ ì¤‘: {self.model_path}")
                self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
                self.model = AutoModelForTokenClassification.from_pretrained(self.model_path)

            self.model.eval()
            self.id2label = {int(k): v for k, v in self.model.config.id2label.items()}
            print(f"âœ… NER ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! ë¼ë²¨ ìˆ˜: {len(self.id2label)}")
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ğŸ’¡ ë”ë¯¸ ëª¨ë¸ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤...")
            self._create_dummy_model()

    def _create_dummy_model(self):
        self.id2label = {
            0: 'O', 1: 'B-PER', 2: 'I-PER', 3: 'B-ORG', 4: 'I-ORG',
            5: 'B-LOC', 6: 'I-LOC', 7: 'B-DATE', 8: 'I-DATE',
            9: 'B-DISEASE', 10: 'I-DISEASE', 11: 'B-CONTACT', 12: 'I-CONTACT'
        }
        print("âš ï¸  ë”ë¯¸ NER ëª¨ë¸ ì‚¬ìš© ì¤‘ (ì‹¤ì œ ëª¨ë¸ ê²½ë¡œë¥¼ ì„¤ì •í•˜ì„¸ìš”)")

    def predict(self, sentence: str) -> List[NERResult]:
        if self.model is None:
            return self._dummy_predict(sentence)

        tokens = sentence.split()
        enc = self.tokenizer(
            tokens, is_split_into_words=True,
            return_tensors="pt", padding="max_length",
            truncation=True, max_length=128
        )

        with torch.no_grad():
            logits = self.model(**enc).logits

        preds = logits.argmax(-1)[0].tolist()
        word_ids = enc.word_ids()

        results = []
        last_word_id = None

        for i, word_id in enumerate(word_ids):
            if word_id is None or word_id == last_word_id:
                continue

            # raw ë¼ë²¨ ë³€í™˜ (PER_B -> B-PER ë“±)
            raw_label = self.id2label[preds[i]]
            if "_" in raw_label:
                tag, pos = raw_label.split("_")
                entity_label = f"{pos}-{tag}"
            else:
                entity_label = raw_label

            results.append(NERResult(
                token=tokens[word_id],
                entity=entity_label,
                start_pos=0,
                end_pos=len(tokens[word_id])
            ))
            last_word_id = word_id

        return results

    def _dummy_predict(self, sentence: str) -> List[NERResult]:
        tokens = sentence.split()
        results = []
        for token in tokens:
            entity = 'O'
            if any(name in token for name in ['ê¹€','ë°•','ì´','ìµœ','ì •','í•œ']):
                entity = 'B-PER'
            elif any(org in token for org in ['ë³‘ì›','ì˜ë£Œì›','ì„¼í„°']):
                entity = 'B-ORG'
            elif any(date in token for date in ['ë…„','ì›”','ì¼']):
                entity = 'B-DATE'
            elif '010-' in token or '02-' in token:
                entity = 'B-CONTACT'
            results.append(NERResult(token=token, entity=entity))
        return results

# ================== 2ë‹¨ê³„: Copula ìœ„í—˜ë„ ë¶„ì„ ==================
class CopulaRiskAnalyzer:
    def __init__(self):
        self._setup_copula_model()
        self.token_to_feature = {
            'ì„œìš¸ëŒ€ë³‘ì›': {'ê¸°ê´€_ì„œìš¸ëŒ€ë³‘ì›':1,'ê¸°ê´€_ì‚¼ì„±ì„œìš¸':0,'ê¸°ê´€_ì—°ì„¸ì˜ë£Œì›':0},
            'ì‚¼ì„±ì„œìš¸ë³‘ì›': {'ê¸°ê´€_ì„œìš¸ëŒ€ë³‘ì›':0,'ê¸°ê´€_ì‚¼ì„±ì„œìš¸':1,'ê¸°ê´€_ì—°ì„¸ì˜ë£Œì›':0},
            'ê°„ì•”': {'ì§ˆë³‘_ê°„ì•”':1,'ì§ˆë³‘_ë°±í˜ˆë³‘':0,'ì§ˆë³‘_ê³ í˜ˆì••':0},
            'ë°±í˜ˆë³‘': {'ì§ˆë³‘_ê°„ì•”':0,'ì§ˆë³‘_ë°±í˜ˆë³‘':1,'ì§ˆë³‘_ê³ í˜ˆì••':0},
            '2023ë…„': {'ë‚ ì§œ_2023ë…„':1,'ë‚ ì§œ_2022ë…„':0,'ë‚ ì§œ_2021ë…„':0},
            '2024ë…„': {'ë‚ ì§œ_2023ë…„':0,'ë‚ ì§œ_2022ë…„':1,'ë‚ ì§œ_2021ë…„':0},
        }

    def _setup_copula_model(self):
        np.random.seed(42)
        df_sample = pd.DataFrame({
            'ê¸°ê´€': np.random.choice(['ì„œìš¸ëŒ€ë³‘ì›','ì‚¼ì„±ì„œìš¸','ì—°ì„¸ì˜ë£Œì›'],1000),
            'ì§ˆë³‘': np.random.choice(['ê°„ì•”','ë°±í˜ˆë³‘','ê³ í˜ˆì••'],1000),
            'ë‚ ì§œ': np.random.choice(['2023ë…„','2022ë…„','2021ë…„'],1000)
        })
        df_encoded = pd.get_dummies(df_sample).astype(float)
        self.copula_model = GaussianMultivariate()
        self.copula_model.fit(df_encoded)
        # ìƒ˜í”Œ ìˆ˜ë¥¼ 1000ìœ¼ë¡œ ì¤„ì—¬ ì†ë„ ê°œì„ 
        self.samples = self.copula_model.sample(1000).round()

    def calculate_risk_weights(self, ner_results: List[NERResult]) -> List[RiskWeight]:
        risk_weights = []
        for ner in ner_results:
            category = self._categorize_entity(ner.entity)
            rw = self._calculate_single_risk(ner.token, ner.entity, category)
            feature = None
            if ner.token in self.token_to_feature:
                feature = ", ".join(self.token_to_feature[ner.token].keys())
            risk_weights.append(RiskWeight(
                token=ner.token, entity=ner.entity,
                category=category, risk_weight=rw,
                copula_feature=feature
            ))
        return risk_weights

    def _categorize_entity(self, entity: str) -> str:
        direct = ['B-PER','I-PER','B-CONTACT','I-CONTACT']
        indirect = ['B-ORG','I-ORG','B-LOC','I-LOC','B-DATE','I-DATE','B-DISEASE','I-DISEASE']
        if entity in direct: return 'ì§ì ‘'
        if entity in indirect: return 'ê°„ì ‘'
        return 'ê¸°íƒ€'

    def _calculate_single_risk(self, token: str, entity: str, category: str) -> int:
        if category=='ì§ì ‘': return 100
        if category=='ê°„ì ‘':
            feat = self.token_to_feature.get(token)
            if feat:
                return round(self._calculate_copula_risk(feat)*100)
            return 30
        return 0

    def _calculate_copula_risk(self, feat: Dict) -> float:
        match = (self.samples[list(feat.keys())]==pd.Series(feat)).all(axis=1)
        prob = match.sum()/len(self.samples)
        return 1-prob

# ================== 3ë‹¨ê³„: ë¬¸ë§¥ì  ìœ„í—˜ ë¶„ì„ ==================
class ContextualRiskAnalyzer:
    def __init__(self):
        self.high_risk_combinations = {('PER','ORG','DATE'):1.5,('PER','DISEASE','ORG'):1.8,('PER','CONTACT'):2.0,('ORG','DATE','DISEASE'):1.3}
        self.medical_risk_keywords = {'ì§„ë‹¨':1.2,'ìˆ˜ìˆ ':1.2,'ì…ì›':1.2,'ì¹˜ë£Œ':1.2,'ì•”':1.3,'ì¢…ì–‘':1.3,'ì§ˆí™˜':1.3,'ì‘ê¸‰':1.5,'ì¤‘í™˜ì':1.5}

    def analyze_contextual_risk(self, text: str, risk_weights: List[RiskWeight]) -> List[RiskWeight]:
        types = [rw.entity[2:] for rw in risk_weights if rw.entity!='O']
        comb_mult = self._get_combination_multiplier(types)
        kw_mult = self._get_keyword_multiplier(text)
        adjusted = []
        for rw in risk_weights:
            w = rw.risk_weight
            if w>0:
                w = min(100, int(w*comb_mult*kw_mult))
            adjusted.append(RiskWeight(rw.token,rw.entity,rw.category,w,rw.copula_feature))
        return adjusted

    def _get_combination_multiplier(self, types: List[str]) -> float:
        m=1.0
        for pat, mult in self.high_risk_combinations.items():
            if set(pat).issubset(set(types)):
                m = max(m, mult)
        return m

    def _get_keyword_multiplier(self, text: str) -> float:
        m=1.0
        for kw, mult in self.medical_risk_keywords.items():
            if kw in text: m = max(m, mult)
        return m

# ================== 4ë‹¨ê³„: ë§ˆìŠ¤í‚¹ ì‹¤í–‰ ==================
class MaskingExecutor:
    def __init__(self, threshold: int = 50):
        self.threshold = threshold
        self.mask_patterns = {'PER':'[PERSON]','ORG':'[HOSPITAL]','LOC':'[LOCATION]','DATE':'[DATE]',
                              'DISEASE':'[DISEASE]','CONTACT':'[CONTACT]','CVL':'[TITLE]','NUM':'[NUMBER]','default':'[MASKED]'}

    def execute_masking(self, text: str, risk_weights: List[RiskWeight]) -> MaskingResult:
        masked_text = text
        log = []
        masked_count = 0
        total = len([rw for rw in risk_weights if rw.entity!='O'])
        for rw in sorted(risk_weights, key=lambda x: x.risk_weight, reverse=True):
            if rw.risk_weight>=self.threshold and rw.entity!='O':
                et = rw.entity[2:]
                pat = self.mask_patterns.get(et, self.mask_patterns['default'])
                if rw.token in masked_text:
                    masked_text = masked_text.replace(rw.token, pat, 1)
                    masked_count+=1
                    log.append({'token':rw.token,'entity':rw.entity,'risk_weight':rw.risk_weight,'masked_as':pat,'reason':f'ìœ„í—˜ë„ {rw.risk_weight} >= ì„ê³„ê°’ {self.threshold}'})
        return MaskingResult(text, masked_text, log, total, masked_count)

# ================== ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•© ==================
class CompleteMedicalDeidentificationPipeline:
    def __init__(self, model_path: str=None, threshold: int=50, use_contextual_analysis: bool=True):
        print("ğŸš€ ì˜ë£Œ í…ìŠ¤íŠ¸ ë¹„ì‹ë³„í™” íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì¤‘...")
        self.ner_model = TrainedNERModel(model_path) if model_path else TrainedNERModel("dummy")
        self.copula_analyzer = CopulaRiskAnalyzer()
        self.contextual_analyzer = ContextualRiskAnalyzer() if use_contextual_analysis else None
        self.masking_executor = MaskingExecutor(threshold)
        print("âœ… íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ!")

    def process(self, text: str, verbose: bool=True) -> MaskingResult:
        if verbose: print(f"\nğŸ“ ì²˜ë¦¬í•  í…ìŠ¤íŠ¸: {text}")
        ner_results = self.ner_model.predict(text)
        if verbose: print(f"ğŸ” 1ë‹¨ê³„ NER ê²°ê³¼: {[(r.token, r.entity) for r in ner_results]}")
        risk_weights = self.copula_analyzer.calculate_risk_weights(ner_results)
        if verbose: print(f"ğŸ“Š 2ë‹¨ê³„ ìœ„í—˜ë„: {[(r.token,r.risk_weight) for r in risk_weights if r.risk_weight>0]}")
        if self.contextual_analyzer:
            risk_weights = self.contextual_analyzer.analyze_contextual_risk(text, risk_weights)
            if verbose: print(f"ğŸ”„ 3ë‹¨ê³„ ì¡°ì •ëœ ìœ„í—˜ë„: {[(r.token,r.risk_weight) for r in risk_weights if r.risk_weight>0]}")
        result = self.masking_executor.execute_masking(text, risk_weights)
        if verbose: print(f"ğŸ­ 4ë‹¨ê³„ ë§ˆìŠ¤í‚¹ ê²°ê³¼: {result.masked_text}")
        return result

    def print_detailed_analysis(self, result: MaskingResult):
        print("\n"+"="*80)
        print("ğŸ¥ ì˜ë£Œ í…ìŠ¤íŠ¸ ë¹„ì‹ë³„í™” ë¶„ì„ ê²°ê³¼")
        print("="*80)
        print(f"ğŸ“ ì›ë¬¸: {result.original_text}")
        print(f"ğŸ­ ë§ˆìŠ¤í‚¹: {result.masked_text}")
        print(f"ğŸ“Š í†µê³„: {result.masked_entities}/{result.total_entities} ê°œì²´ ë§ˆìŠ¤í‚¹")
        if result.masking_log:
            print("\nğŸ“‹ ë§ˆìŠ¤í‚¹ ìƒì„¸ ë¡œê·¸:")
            for i, log in enumerate(result.masking_log, 1):
                print(f"  {i}. '{log['token']}' â†’ {log['masked_as']} (ìœ„í—˜ë„: {log['risk_weight']})")
        else:
            print("\nâœ… ë§ˆìŠ¤í‚¹ì´ í•„ìš”í•œ ê³ ìœ„í—˜ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ================== í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ==================
def main():
    print("ğŸš€ ì™„ì „í•œ ì˜ë£Œ í…ìŠ¤íŠ¸ ë¹„ì‹ë³„í™” íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸")
    # ë³€ê²½ëœ model_path ì§€ì •
    model_path = "ner-koelectra-lora-merged"
    pipeline = CompleteMedicalDeidentificationPipeline(
        model_path=model_path,
        threshold=50,
        use_contextual_analysis=True
    )

    test_cases = [
        "ê¹€ì² ìˆ˜ì”¨ê°€ 2023ë…„ 10ì›”ì— ì„œìš¸ëŒ€ë³‘ì›ì—ì„œ ê°„ì•” ì§„ë‹¨ì„ ë°›ì•˜ìŠµë‹ˆë‹¤.",
        "ë°•ì˜í¬(010-1234-5678)ëŠ” ì‚¼ì„±ì„œìš¸ë³‘ì›ì—ì„œ ìˆ˜ìˆ ì„ ë°›ì•˜ë‹¤.",
        "í™˜ìëŠ” ë‚´ì¼ ê²€ì‚¬ë¥¼ ë°›ì„ ì˜ˆì •ì…ë‹ˆë‹¤.",
        "ì´ìˆœì‹  êµìˆ˜ëŠ” ì—°ì„¸ì˜ë£Œì›ì—ì„œ ë°±í˜ˆë³‘ ì—°êµ¬ë¥¼ í•˜ê³  ìˆë‹¤."
    ]

    for i, text in enumerate(test_cases, 1):
        print(f"\n{'='*20} í…ŒìŠ¤íŠ¸ {i} {'='*20}")
        result = pipeline.process(text, verbose=True)
        pipeline.print_detailed_analysis(result)

    print("\n"+"="*80)
    print("âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
